{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:embedding_expansion--> param.shape:torch.Size([384, 192])\n",
      "name:depth_expansion_operator--> param.shape:torch.Size([8, 12, 12])\n",
      "name:small_model.cls_token--> param.shape:torch.Size([1, 1, 192])\n",
      "name:small_model.pos_embed--> param.shape:torch.Size([1, 197, 192])\n",
      "name:small_model.patch_embed.proj.weight--> param.shape:torch.Size([192, 3, 16, 16])\n",
      "name:small_model.patch_embed.proj.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.0.norm1.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.0.norm1.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.0.attn.qkv.weight--> param.shape:torch.Size([576, 192])\n",
      "name:small_model.blocks.0.attn.qkv.bias--> param.shape:torch.Size([576])\n",
      "name:small_model.blocks.0.attn.proj.weight--> param.shape:torch.Size([192, 192])\n",
      "name:small_model.blocks.0.attn.proj.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.0.norm2.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.0.norm2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.0.mlp.fc1.weight--> param.shape:torch.Size([768, 192])\n",
      "name:small_model.blocks.0.mlp.fc1.bias--> param.shape:torch.Size([768])\n",
      "name:small_model.blocks.0.mlp.fc2.weight--> param.shape:torch.Size([192, 768])\n",
      "name:small_model.blocks.0.mlp.fc2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.1.norm1.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.1.norm1.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.1.attn.qkv.weight--> param.shape:torch.Size([576, 192])\n",
      "name:small_model.blocks.1.attn.qkv.bias--> param.shape:torch.Size([576])\n",
      "name:small_model.blocks.1.attn.proj.weight--> param.shape:torch.Size([192, 192])\n",
      "name:small_model.blocks.1.attn.proj.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.1.norm2.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.1.norm2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.1.mlp.fc1.weight--> param.shape:torch.Size([768, 192])\n",
      "name:small_model.blocks.1.mlp.fc1.bias--> param.shape:torch.Size([768])\n",
      "name:small_model.blocks.1.mlp.fc2.weight--> param.shape:torch.Size([192, 768])\n",
      "name:small_model.blocks.1.mlp.fc2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.2.norm1.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.2.norm1.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.2.attn.qkv.weight--> param.shape:torch.Size([576, 192])\n",
      "name:small_model.blocks.2.attn.qkv.bias--> param.shape:torch.Size([576])\n",
      "name:small_model.blocks.2.attn.proj.weight--> param.shape:torch.Size([192, 192])\n",
      "name:small_model.blocks.2.attn.proj.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.2.norm2.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.2.norm2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.2.mlp.fc1.weight--> param.shape:torch.Size([768, 192])\n",
      "name:small_model.blocks.2.mlp.fc1.bias--> param.shape:torch.Size([768])\n",
      "name:small_model.blocks.2.mlp.fc2.weight--> param.shape:torch.Size([192, 768])\n",
      "name:small_model.blocks.2.mlp.fc2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.3.norm1.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.3.norm1.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.3.attn.qkv.weight--> param.shape:torch.Size([576, 192])\n",
      "name:small_model.blocks.3.attn.qkv.bias--> param.shape:torch.Size([576])\n",
      "name:small_model.blocks.3.attn.proj.weight--> param.shape:torch.Size([192, 192])\n",
      "name:small_model.blocks.3.attn.proj.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.3.norm2.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.3.norm2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.3.mlp.fc1.weight--> param.shape:torch.Size([768, 192])\n",
      "name:small_model.blocks.3.mlp.fc1.bias--> param.shape:torch.Size([768])\n",
      "name:small_model.blocks.3.mlp.fc2.weight--> param.shape:torch.Size([192, 768])\n",
      "name:small_model.blocks.3.mlp.fc2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.4.norm1.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.4.norm1.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.4.attn.qkv.weight--> param.shape:torch.Size([576, 192])\n",
      "name:small_model.blocks.4.attn.qkv.bias--> param.shape:torch.Size([576])\n",
      "name:small_model.blocks.4.attn.proj.weight--> param.shape:torch.Size([192, 192])\n",
      "name:small_model.blocks.4.attn.proj.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.4.norm2.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.4.norm2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.4.mlp.fc1.weight--> param.shape:torch.Size([768, 192])\n",
      "name:small_model.blocks.4.mlp.fc1.bias--> param.shape:torch.Size([768])\n",
      "name:small_model.blocks.4.mlp.fc2.weight--> param.shape:torch.Size([192, 768])\n",
      "name:small_model.blocks.4.mlp.fc2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.5.norm1.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.5.norm1.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.5.attn.qkv.weight--> param.shape:torch.Size([576, 192])\n",
      "name:small_model.blocks.5.attn.qkv.bias--> param.shape:torch.Size([576])\n",
      "name:small_model.blocks.5.attn.proj.weight--> param.shape:torch.Size([192, 192])\n",
      "name:small_model.blocks.5.attn.proj.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.5.norm2.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.5.norm2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.5.mlp.fc1.weight--> param.shape:torch.Size([768, 192])\n",
      "name:small_model.blocks.5.mlp.fc1.bias--> param.shape:torch.Size([768])\n",
      "name:small_model.blocks.5.mlp.fc2.weight--> param.shape:torch.Size([192, 768])\n",
      "name:small_model.blocks.5.mlp.fc2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.6.norm1.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.6.norm1.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.6.attn.qkv.weight--> param.shape:torch.Size([576, 192])\n",
      "name:small_model.blocks.6.attn.qkv.bias--> param.shape:torch.Size([576])\n",
      "name:small_model.blocks.6.attn.proj.weight--> param.shape:torch.Size([192, 192])\n",
      "name:small_model.blocks.6.attn.proj.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.6.norm2.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.6.norm2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.6.mlp.fc1.weight--> param.shape:torch.Size([768, 192])\n",
      "name:small_model.blocks.6.mlp.fc1.bias--> param.shape:torch.Size([768])\n",
      "name:small_model.blocks.6.mlp.fc2.weight--> param.shape:torch.Size([192, 768])\n",
      "name:small_model.blocks.6.mlp.fc2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.7.norm1.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.7.norm1.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.7.attn.qkv.weight--> param.shape:torch.Size([576, 192])\n",
      "name:small_model.blocks.7.attn.qkv.bias--> param.shape:torch.Size([576])\n",
      "name:small_model.blocks.7.attn.proj.weight--> param.shape:torch.Size([192, 192])\n",
      "name:small_model.blocks.7.attn.proj.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.7.norm2.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.7.norm2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.7.mlp.fc1.weight--> param.shape:torch.Size([768, 192])\n",
      "name:small_model.blocks.7.mlp.fc1.bias--> param.shape:torch.Size([768])\n",
      "name:small_model.blocks.7.mlp.fc2.weight--> param.shape:torch.Size([192, 768])\n",
      "name:small_model.blocks.7.mlp.fc2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.8.norm1.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.8.norm1.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.8.attn.qkv.weight--> param.shape:torch.Size([576, 192])\n",
      "name:small_model.blocks.8.attn.qkv.bias--> param.shape:torch.Size([576])\n",
      "name:small_model.blocks.8.attn.proj.weight--> param.shape:torch.Size([192, 192])\n",
      "name:small_model.blocks.8.attn.proj.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.8.norm2.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.8.norm2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.8.mlp.fc1.weight--> param.shape:torch.Size([768, 192])\n",
      "name:small_model.blocks.8.mlp.fc1.bias--> param.shape:torch.Size([768])\n",
      "name:small_model.blocks.8.mlp.fc2.weight--> param.shape:torch.Size([192, 768])\n",
      "name:small_model.blocks.8.mlp.fc2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.9.norm1.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.9.norm1.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.9.attn.qkv.weight--> param.shape:torch.Size([576, 192])\n",
      "name:small_model.blocks.9.attn.qkv.bias--> param.shape:torch.Size([576])\n",
      "name:small_model.blocks.9.attn.proj.weight--> param.shape:torch.Size([192, 192])\n",
      "name:small_model.blocks.9.attn.proj.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.9.norm2.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.9.norm2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.9.mlp.fc1.weight--> param.shape:torch.Size([768, 192])\n",
      "name:small_model.blocks.9.mlp.fc1.bias--> param.shape:torch.Size([768])\n",
      "name:small_model.blocks.9.mlp.fc2.weight--> param.shape:torch.Size([192, 768])\n",
      "name:small_model.blocks.9.mlp.fc2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.10.norm1.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.10.norm1.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.10.attn.qkv.weight--> param.shape:torch.Size([576, 192])\n",
      "name:small_model.blocks.10.attn.qkv.bias--> param.shape:torch.Size([576])\n",
      "name:small_model.blocks.10.attn.proj.weight--> param.shape:torch.Size([192, 192])\n",
      "name:small_model.blocks.10.attn.proj.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.10.norm2.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.10.norm2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.10.mlp.fc1.weight--> param.shape:torch.Size([768, 192])\n",
      "name:small_model.blocks.10.mlp.fc1.bias--> param.shape:torch.Size([768])\n",
      "name:small_model.blocks.10.mlp.fc2.weight--> param.shape:torch.Size([192, 768])\n",
      "name:small_model.blocks.10.mlp.fc2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.11.norm1.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.11.norm1.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.11.attn.qkv.weight--> param.shape:torch.Size([576, 192])\n",
      "name:small_model.blocks.11.attn.qkv.bias--> param.shape:torch.Size([576])\n",
      "name:small_model.blocks.11.attn.proj.weight--> param.shape:torch.Size([192, 192])\n",
      "name:small_model.blocks.11.attn.proj.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.11.norm2.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.11.norm2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.11.mlp.fc1.weight--> param.shape:torch.Size([768, 192])\n",
      "name:small_model.blocks.11.mlp.fc1.bias--> param.shape:torch.Size([768])\n",
      "name:small_model.blocks.11.mlp.fc2.weight--> param.shape:torch.Size([192, 768])\n",
      "name:small_model.blocks.11.mlp.fc2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.norm.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.norm.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.head.weight--> param.shape:torch.Size([100, 192])\n",
      "name:small_model.head.bias--> param.shape:torch.Size([100])\n",
      "name:large_model.cls_token--> param.shape:torch.Size([1, 1, 384])\n",
      "name:large_model.pos_embed--> param.shape:torch.Size([1, 197, 384])\n",
      "name:large_model.patch_embed.proj.weight--> param.shape:torch.Size([384, 3, 16, 16])\n",
      "name:large_model.patch_embed.proj.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.0.norm1.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.0.norm1.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.0.attn.qkv.weight--> param.shape:torch.Size([1152, 384])\n",
      "name:large_model.blocks.0.attn.qkv.bias--> param.shape:torch.Size([1152])\n",
      "name:large_model.blocks.0.attn.proj.weight--> param.shape:torch.Size([384, 384])\n",
      "name:large_model.blocks.0.attn.proj.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.0.norm2.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.0.norm2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.0.mlp.fc1.weight--> param.shape:torch.Size([1536, 384])\n",
      "name:large_model.blocks.0.mlp.fc1.bias--> param.shape:torch.Size([1536])\n",
      "name:large_model.blocks.0.mlp.fc2.weight--> param.shape:torch.Size([384, 1536])\n",
      "name:large_model.blocks.0.mlp.fc2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.1.norm1.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.1.norm1.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.1.attn.qkv.weight--> param.shape:torch.Size([1152, 384])\n",
      "name:large_model.blocks.1.attn.qkv.bias--> param.shape:torch.Size([1152])\n",
      "name:large_model.blocks.1.attn.proj.weight--> param.shape:torch.Size([384, 384])\n",
      "name:large_model.blocks.1.attn.proj.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.1.norm2.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.1.norm2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.1.mlp.fc1.weight--> param.shape:torch.Size([1536, 384])\n",
      "name:large_model.blocks.1.mlp.fc1.bias--> param.shape:torch.Size([1536])\n",
      "name:large_model.blocks.1.mlp.fc2.weight--> param.shape:torch.Size([384, 1536])\n",
      "name:large_model.blocks.1.mlp.fc2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.2.norm1.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.2.norm1.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.2.attn.qkv.weight--> param.shape:torch.Size([1152, 384])\n",
      "name:large_model.blocks.2.attn.qkv.bias--> param.shape:torch.Size([1152])\n",
      "name:large_model.blocks.2.attn.proj.weight--> param.shape:torch.Size([384, 384])\n",
      "name:large_model.blocks.2.attn.proj.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.2.norm2.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.2.norm2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.2.mlp.fc1.weight--> param.shape:torch.Size([1536, 384])\n",
      "name:large_model.blocks.2.mlp.fc1.bias--> param.shape:torch.Size([1536])\n",
      "name:large_model.blocks.2.mlp.fc2.weight--> param.shape:torch.Size([384, 1536])\n",
      "name:large_model.blocks.2.mlp.fc2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.3.norm1.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.3.norm1.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.3.attn.qkv.weight--> param.shape:torch.Size([1152, 384])\n",
      "name:large_model.blocks.3.attn.qkv.bias--> param.shape:torch.Size([1152])\n",
      "name:large_model.blocks.3.attn.proj.weight--> param.shape:torch.Size([384, 384])\n",
      "name:large_model.blocks.3.attn.proj.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.3.norm2.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.3.norm2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.3.mlp.fc1.weight--> param.shape:torch.Size([1536, 384])\n",
      "name:large_model.blocks.3.mlp.fc1.bias--> param.shape:torch.Size([1536])\n",
      "name:large_model.blocks.3.mlp.fc2.weight--> param.shape:torch.Size([384, 1536])\n",
      "name:large_model.blocks.3.mlp.fc2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.4.norm1.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.4.norm1.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.4.attn.qkv.weight--> param.shape:torch.Size([1152, 384])\n",
      "name:large_model.blocks.4.attn.qkv.bias--> param.shape:torch.Size([1152])\n",
      "name:large_model.blocks.4.attn.proj.weight--> param.shape:torch.Size([384, 384])\n",
      "name:large_model.blocks.4.attn.proj.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.4.norm2.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.4.norm2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.4.mlp.fc1.weight--> param.shape:torch.Size([1536, 384])\n",
      "name:large_model.blocks.4.mlp.fc1.bias--> param.shape:torch.Size([1536])\n",
      "name:large_model.blocks.4.mlp.fc2.weight--> param.shape:torch.Size([384, 1536])\n",
      "name:large_model.blocks.4.mlp.fc2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.5.norm1.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.5.norm1.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.5.attn.qkv.weight--> param.shape:torch.Size([1152, 384])\n",
      "name:large_model.blocks.5.attn.qkv.bias--> param.shape:torch.Size([1152])\n",
      "name:large_model.blocks.5.attn.proj.weight--> param.shape:torch.Size([384, 384])\n",
      "name:large_model.blocks.5.attn.proj.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.5.norm2.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.5.norm2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.5.mlp.fc1.weight--> param.shape:torch.Size([1536, 384])\n",
      "name:large_model.blocks.5.mlp.fc1.bias--> param.shape:torch.Size([1536])\n",
      "name:large_model.blocks.5.mlp.fc2.weight--> param.shape:torch.Size([384, 1536])\n",
      "name:large_model.blocks.5.mlp.fc2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.6.norm1.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.6.norm1.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.6.attn.qkv.weight--> param.shape:torch.Size([1152, 384])\n",
      "name:large_model.blocks.6.attn.qkv.bias--> param.shape:torch.Size([1152])\n",
      "name:large_model.blocks.6.attn.proj.weight--> param.shape:torch.Size([384, 384])\n",
      "name:large_model.blocks.6.attn.proj.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.6.norm2.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.6.norm2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.6.mlp.fc1.weight--> param.shape:torch.Size([1536, 384])\n",
      "name:large_model.blocks.6.mlp.fc1.bias--> param.shape:torch.Size([1536])\n",
      "name:large_model.blocks.6.mlp.fc2.weight--> param.shape:torch.Size([384, 1536])\n",
      "name:large_model.blocks.6.mlp.fc2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.7.norm1.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.7.norm1.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.7.attn.qkv.weight--> param.shape:torch.Size([1152, 384])\n",
      "name:large_model.blocks.7.attn.qkv.bias--> param.shape:torch.Size([1152])\n",
      "name:large_model.blocks.7.attn.proj.weight--> param.shape:torch.Size([384, 384])\n",
      "name:large_model.blocks.7.attn.proj.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.7.norm2.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.7.norm2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.7.mlp.fc1.weight--> param.shape:torch.Size([1536, 384])\n",
      "name:large_model.blocks.7.mlp.fc1.bias--> param.shape:torch.Size([1536])\n",
      "name:large_model.blocks.7.mlp.fc2.weight--> param.shape:torch.Size([384, 1536])\n",
      "name:large_model.blocks.7.mlp.fc2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.8.norm1.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.8.norm1.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.8.attn.qkv.weight--> param.shape:torch.Size([1152, 384])\n",
      "name:large_model.blocks.8.attn.qkv.bias--> param.shape:torch.Size([1152])\n",
      "name:large_model.blocks.8.attn.proj.weight--> param.shape:torch.Size([384, 384])\n",
      "name:large_model.blocks.8.attn.proj.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.8.norm2.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.8.norm2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.8.mlp.fc1.weight--> param.shape:torch.Size([1536, 384])\n",
      "name:large_model.blocks.8.mlp.fc1.bias--> param.shape:torch.Size([1536])\n",
      "name:large_model.blocks.8.mlp.fc2.weight--> param.shape:torch.Size([384, 1536])\n",
      "name:large_model.blocks.8.mlp.fc2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.9.norm1.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.9.norm1.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.9.attn.qkv.weight--> param.shape:torch.Size([1152, 384])\n",
      "name:large_model.blocks.9.attn.qkv.bias--> param.shape:torch.Size([1152])\n",
      "name:large_model.blocks.9.attn.proj.weight--> param.shape:torch.Size([384, 384])\n",
      "name:large_model.blocks.9.attn.proj.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.9.norm2.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.9.norm2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.9.mlp.fc1.weight--> param.shape:torch.Size([1536, 384])\n",
      "name:large_model.blocks.9.mlp.fc1.bias--> param.shape:torch.Size([1536])\n",
      "name:large_model.blocks.9.mlp.fc2.weight--> param.shape:torch.Size([384, 1536])\n",
      "name:large_model.blocks.9.mlp.fc2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.10.norm1.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.10.norm1.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.10.attn.qkv.weight--> param.shape:torch.Size([1152, 384])\n",
      "name:large_model.blocks.10.attn.qkv.bias--> param.shape:torch.Size([1152])\n",
      "name:large_model.blocks.10.attn.proj.weight--> param.shape:torch.Size([384, 384])\n",
      "name:large_model.blocks.10.attn.proj.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.10.norm2.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.10.norm2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.10.mlp.fc1.weight--> param.shape:torch.Size([1536, 384])\n",
      "name:large_model.blocks.10.mlp.fc1.bias--> param.shape:torch.Size([1536])\n",
      "name:large_model.blocks.10.mlp.fc2.weight--> param.shape:torch.Size([384, 1536])\n",
      "name:large_model.blocks.10.mlp.fc2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.11.norm1.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.11.norm1.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.11.attn.qkv.weight--> param.shape:torch.Size([1152, 384])\n",
      "name:large_model.blocks.11.attn.qkv.bias--> param.shape:torch.Size([1152])\n",
      "name:large_model.blocks.11.attn.proj.weight--> param.shape:torch.Size([384, 384])\n",
      "name:large_model.blocks.11.attn.proj.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.11.norm2.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.11.norm2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.blocks.11.mlp.fc1.weight--> param.shape:torch.Size([1536, 384])\n",
      "name:large_model.blocks.11.mlp.fc1.bias--> param.shape:torch.Size([1536])\n",
      "name:large_model.blocks.11.mlp.fc2.weight--> param.shape:torch.Size([384, 1536])\n",
      "name:large_model.blocks.11.mlp.fc2.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.norm.weight--> param.shape:torch.Size([384])\n",
      "name:large_model.norm.bias--> param.shape:torch.Size([384])\n",
      "name:large_model.head.weight--> param.shape:torch.Size([100, 384])\n",
      "name:large_model.head.bias--> param.shape:torch.Size([100])\n",
      "name:width_expansion_operator.Weight_B_Q_0--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_K_0--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_V_0--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_fc1_0--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_Q_1--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_K_1--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_V_1--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_fc1_1--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_Q_2--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_K_2--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_V_2--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_fc1_2--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_Q_3--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_K_3--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_V_3--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_fc1_3--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_Q_4--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_K_4--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_V_4--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_fc1_4--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_Q_5--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_K_5--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_V_5--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_fc1_5--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_Q_6--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_K_6--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_V_6--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_fc1_6--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_Q_7--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_K_7--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_V_7--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_fc1_7--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_Q_8--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_K_8--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_V_8--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_fc1_8--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_Q_9--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_K_9--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_V_9--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_fc1_9--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_Q_10--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_K_10--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_V_10--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_fc1_10--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_Q_11--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_K_11--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_V_11--> param.shape:torch.Size([384, 192])\n",
      "name:width_expansion_operator.Weight_B_fc1_11--> param.shape:torch.Size([384, 192])\n"
     ]
    }
   ],
   "source": [
    "# Test LiGO VIT\n",
    "from models.ViT import LiGOViT\n",
    "\n",
    "params = dict(\n",
    "    patch_size=16,\n",
    "    n_hiddens=192,\n",
    "    n_layers=12,\n",
    "    num_heads=3,\n",
    "    target_hiddens=384,\n",
    "    target_layers=12,\n",
    "    target_heads=4,\n",
    "    small_model_path=None,\n",
    "    num_classes=100,\n",
    ")\n",
    "\n",
    "model = LiGOViT(**params)\n",
    "for name,param in model.named_parameters():\n",
    "    print(\"name:{}--> param.shape:{}\".format(name,param.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:small_model.cls_token--> param.shape:torch.Size([1, 1, 192])\n",
      "name:small_model.pos_embed--> param.shape:torch.Size([1, 197, 192])\n",
      "name:small_model.patch_embed.proj.weight--> param.shape:torch.Size([192, 3, 16, 16])\n",
      "name:small_model.patch_embed.proj.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.0.norm1.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.0.norm1.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.0.attn.qkv.weight--> param.shape:torch.Size([576, 192])\n",
      "name:small_model.blocks.0.attn.qkv.bias--> param.shape:torch.Size([576])\n",
      "name:small_model.blocks.0.attn.proj.weight--> param.shape:torch.Size([192, 192])\n",
      "name:small_model.blocks.0.attn.proj.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.0.norm2.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.0.norm2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.0.mlp.fc1.weight--> param.shape:torch.Size([768, 192])\n",
      "name:small_model.blocks.0.mlp.fc1.bias--> param.shape:torch.Size([768])\n",
      "name:small_model.blocks.0.mlp.fc2.weight--> param.shape:torch.Size([192, 768])\n",
      "name:small_model.blocks.0.mlp.fc2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.1.norm1.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.1.norm1.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.1.attn.qkv.weight--> param.shape:torch.Size([576, 192])\n",
      "name:small_model.blocks.1.attn.qkv.bias--> param.shape:torch.Size([576])\n",
      "name:small_model.blocks.1.attn.proj.weight--> param.shape:torch.Size([192, 192])\n",
      "name:small_model.blocks.1.attn.proj.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.1.norm2.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.1.norm2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.1.mlp.fc1.weight--> param.shape:torch.Size([768, 192])\n",
      "name:small_model.blocks.1.mlp.fc1.bias--> param.shape:torch.Size([768])\n",
      "name:small_model.blocks.1.mlp.fc2.weight--> param.shape:torch.Size([192, 768])\n",
      "name:small_model.blocks.1.mlp.fc2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.2.norm1.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.2.norm1.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.2.attn.qkv.weight--> param.shape:torch.Size([576, 192])\n",
      "name:small_model.blocks.2.attn.qkv.bias--> param.shape:torch.Size([576])\n",
      "name:small_model.blocks.2.attn.proj.weight--> param.shape:torch.Size([192, 192])\n",
      "name:small_model.blocks.2.attn.proj.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.2.norm2.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.2.norm2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.2.mlp.fc1.weight--> param.shape:torch.Size([768, 192])\n",
      "name:small_model.blocks.2.mlp.fc1.bias--> param.shape:torch.Size([768])\n",
      "name:small_model.blocks.2.mlp.fc2.weight--> param.shape:torch.Size([192, 768])\n",
      "name:small_model.blocks.2.mlp.fc2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.3.norm1.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.3.norm1.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.3.attn.qkv.weight--> param.shape:torch.Size([576, 192])\n",
      "name:small_model.blocks.3.attn.qkv.bias--> param.shape:torch.Size([576])\n",
      "name:small_model.blocks.3.attn.proj.weight--> param.shape:torch.Size([192, 192])\n",
      "name:small_model.blocks.3.attn.proj.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.3.norm2.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.3.norm2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.blocks.3.mlp.fc1.weight--> param.shape:torch.Size([768, 192])\n",
      "name:small_model.blocks.3.mlp.fc1.bias--> param.shape:torch.Size([768])\n",
      "name:small_model.blocks.3.mlp.fc2.weight--> param.shape:torch.Size([192, 768])\n",
      "name:small_model.blocks.3.mlp.fc2.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.norm.weight--> param.shape:torch.Size([192])\n",
      "name:small_model.norm.bias--> param.shape:torch.Size([192])\n",
      "name:small_model.head.weight--> param.shape:torch.Size([100, 192])\n",
      "name:small_model.head.bias--> param.shape:torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# Test LiGO VIT\n",
    "from models.ViT import LiGOViT\n",
    "\n",
    "params = dict(\n",
    "    patch_size=16,\n",
    "    n_hiddens=192,\n",
    "    n_layers=4,\n",
    "    num_heads=3,\n",
    "    small_model_path=None,\n",
    "    num_classes=100,\n",
    ")\n",
    "\n",
    "model = LiGOViT(**params)\n",
    "for name,param in model.named_parameters():\n",
    "    print(\"name:{}--> param.shape:{}\".format(name,param.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exp_name': 'fed_vit_various2h_iid_noagg',\n",
       " 'log_dir': './logs',\n",
       " 'seed': 1,\n",
       " 'async_training': False,\n",
       " 'num_clients': 5,\n",
       " 'batch_size': 512,\n",
       " 'epochs': 400,\n",
       " 'local_ep': 1,\n",
       " 'device': 'cuda:1',\n",
       " 'aggregation': False,\n",
       " 'small_model_training': False,\n",
       " 'small_model_training_round': 20,\n",
       " 'dataset': 'CIFAR100',\n",
       " 'data_root': './datasets',\n",
       " 'iid_degree': 1000,\n",
       " 'criterion': 'CrossEntropyLoss',\n",
       " 'criterion_kwargs': None,\n",
       " 'optimizer': 'AdamW',\n",
       " 'optimizer_kwargs': {'lr': 0.0001, 'weight_decay': 0.05},\n",
       " 'model': 'LiGOViT',\n",
       " 'various_model': True,\n",
       " 'model_kwargs': {'tiny': {'patch_size': 16,\n",
       "   'n_hiddens': 64,\n",
       "   'n_layers': 5,\n",
       "   'num_heads': 8,\n",
       "   'target_hiddens': 160,\n",
       "   'target_layers': 8,\n",
       "   'target_heads': 8,\n",
       "   'num_classes': 100,\n",
       "   'small_model_path': 'pretrained_models/myvit_tiny_patch16_224/best_checkpoint.pth'},\n",
       "  'small': {'patch_size': 16,\n",
       "   'n_hiddens': 96,\n",
       "   'n_layers': 6,\n",
       "   'num_heads': 8,\n",
       "   'target_hiddens': 160,\n",
       "   'target_layers': 8,\n",
       "   'target_heads': 8,\n",
       "   'num_classes': 100,\n",
       "   'small_model_path': 'pretrained_models/myvit_small_patch16_224/best_checkpoint.pth'},\n",
       "  'large': {'patch_size': 16,\n",
       "   'n_hiddens': 128,\n",
       "   'n_layers': 7,\n",
       "   'num_heads': 8,\n",
       "   'target_hiddens': 160,\n",
       "   'target_layers': 8,\n",
       "   'target_heads': 8,\n",
       "   'num_classes': 100,\n",
       "   'small_model_path': 'pretrained_models/myvit_base_patch16_224/best_checkpoint.pth'}},\n",
       " 'homogeneous_model_kwargs': {'patch_size': 16,\n",
       "  'n_hiddens': 160,\n",
       "  'n_layers': 8,\n",
       "  'num_heads': 8,\n",
       "  'target_hiddens': 192,\n",
       "  'target_layers': 8,\n",
       "  'target_heads': 8,\n",
       "  'num_classes': 100,\n",
       "  'small_model_path': None}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import gen_hetro_model_args\n",
    "import yaml\n",
    "\n",
    "with open(\"configs/vit_config/fed_vit_various2h_iid_noagg.yaml\") as f:\n",
    "    cfg = yaml.load(f,yaml.FullLoader)\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_hetro_model_args(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from data import construct_dataset\n",
    "\n",
    "dataset, testset = construct_dataset(\"CIFAR100\",\"datasets\",10,0.8,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4912"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "gen_hetro_model_args(cfg).__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(gen_hetro_model_args(cfg)).__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedLe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
