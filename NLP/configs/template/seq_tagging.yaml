# Dataset
dataset: wikiner
data_file_path: ../fednlp_data/data_files/wikiner_data.h5
partition_file_path: ../fednlp_data/partition_files/wikiner_partition.h5
partition_method: uniform
reprocess_input_data: False
num_labels: 9
# Model Related
model_type: bert
model_class: SeqTaggingModel
model: bert-base-uncased
do_lower_case: True
# Training Related
batch_size: 16
train_batch_size: 16
eval_batch_size: 16
max_seq_length: 128
n_gpu: 1
fp16: False
manual_seed: 42
# Logging related
output_dir: ./output
# Federated Learning Related
aggregation: True
federated_optimizer: FedAvg
comm_round: 200
is_mobile: 1
client_num_in_total: 10
client_num_per_round: 10
epochs: 10
gradient_accumulation_steps: 1
client_optimizer: AdamW
learning_rate: 0.0001
weight_decay: 0.05
clip_grad_norm: true
evaluate_during_training: False
evaluate_during_training_steps: 100
frequency_of_the_test: 1
max_grad_norm: 1

small_model_training: True
small_model_training_round: 100
# Model kwargs
model_kwargs:
  nano:
    small_model:
      hidden_size: 128
      num_hidden_layers: 2
      intermediate_size: 512
      num_attention_heads: 2
      num_labels: 9
      pretrained_path: pretrained_models/bert_uncased_L-2_H-128_A-2
    large_model:
      hidden_size: 512
      num_hidden_layers: 4
      intermediate_size: 512
      num_attention_heads: 8
      num_labels: 9
  mini:
    small_model: 
      hidden_size: 256
      num_hidden_layers: 4
      intermediate_size: 1024
      num_attention_heads: 4
      num_labels: 9
      pretrained_path: pretrained_models/bert_uncased_L-4_H-256_A-4
    large_model:
      hidden_size: 512
      num_hidden_layers: 4
      intermediate_size: 512
      num_attention_heads: 8
      num_labels: 9
      
  small:
    small_model:
      hidden_size: 512
      num_hidden_layers: 4
      intermediate_size: 2048
      num_attention_heads: 8
      num_labels: 9
      pretrained_path: pretrained_models/bert_uncased_L-4_H-512_A-8
    large_model:
      hidden_size: 512
      num_hidden_layers: 4
      intermediate_size: 512
      num_attention_heads: 8
      num_labels: 9
  # medium:
  #   small_model:
  #     hidden_size: 512
  #     num_hidden_layers: 8
  #     intermediate_size: 2048
  #     num_attention_heads: 8
  #   large_model:
  #     hidden_size: 512
  #     num_hidden_layers: 8
  #     intermediate_size: 2048
  #     num_attention_heads: 8

homogeneous_model_kwargs:
  small_model:
    hidden_size: 512
    num_hidden_layers: 4
    intermediate_size: 512
    num_attention_heads: 4
    num_labels: 9
  large_model:
    hidden_size: 512
    num_hidden_layers: 8
    intermediate_size: 512
    num_attention_heads: 8
    num_labels: 9