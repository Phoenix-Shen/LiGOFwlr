# Dataset
dataset: "mrqa"
data_file_path: ../fednlp_data/data_files/mrqa_data.h5
partition_file_path: ../fednlp_data/partition_files/mrqa_partition.h5
partition_method: niid_label_clients=10_alpha=100
reprocess_input_data: False
num_labels: 2
# Model Related
model_type: bert
model_class: QuestionAnsweringModel
model: bert-base-uncased
do_lower_case: True
# Training Related
freeze_layers: ''
batch_size: 16
eval_batch_size: 8
max_seq_length: 128
n_gpu: 1
fp16: False
manual_seed: 42
# Logging related
output_dir: ./output
# Federated Learning Related
aggregation: True
federated_optimizer: FedAvg
fl_algorithm: ""
comm_round: 200
is_mobile: 1
client_num_in_total: 10
client_num_per_round: 10
epochs: 1
gradient_accumulation_steps: 1
client_optimizer: AdamW
learning_rate: 0.0001
weight_decay: 0.05
clip_grad_norm: true
evaluate_during_training: False
evaluate_during_training_steps: 100
frequency_of_the_test: 1
max_grad_norm: 1

small_model_training: True
small_model_training_round: 100
# Model kwargs
model_kwargs:
  nano:
    small_model:
      hidden_size: 128
      num_hidden_layers: 2
      intermediate_size: 512
      num_attention_heads: 2
      num_labels: 2
      pretrained_path: pretrained_models/bert_uncased_L-2_H-128_A-2
    large_model:
      hidden_size: 512
      num_hidden_layers: 8
      intermediate_size: 512
      num_attention_heads: 8
      num_labels: 2
  mini:
    small_model: 
      hidden_size: 256
      num_hidden_layers: 4
      intermediate_size: 1024
      num_attention_heads: 4
      num_labels: 2
      pretrained_path: pretrained_models/bert_uncased_L-4_H-256_A-4
    large_model:
      hidden_size: 512
      num_hidden_layers: 8
      intermediate_size: 512
      num_attention_heads: 8
      num_labels: 2
  small:
    small_model:
      hidden_size: 512
      num_hidden_layers: 4
      intermediate_size: 2048
      num_attention_heads: 4
      num_labels: 2
      pretrained_path: pretrained_models/bert_uncased_L-4_H-512_A-8
    large_model:
      hidden_size: 512
      num_hidden_layers: 8
      intermediate_size: 512
      num_attention_heads: 8
      num_labels: 2
  # medium:
  #   small_model:
  #     hidden_size: 512
  #     num_hidden_layers: 8
  #     intermediate_size: 2048
  #     num_attention_heads: 8
  #   large_model:
  #     hidden_size: 512
  #     num_hidden_layers: 8
  #     intermediate_size: 2048
  #     num_attention_heads: 8

homogeneous_model_kwargs:
  small_model:
    hidden_size: 512
    num_hidden_layers: 8
    intermediate_size: 512
    num_attention_heads: 8
    num_labels: 2
  large_model:
    hidden_size: 768
    num_hidden_layers: 12
    intermediate_size: 768
    num_attention_heads: 12
    num_labels: 2