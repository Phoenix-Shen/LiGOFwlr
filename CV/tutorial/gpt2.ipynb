{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers for NLP\n",
    "\n",
    "- Encoder-Decoder Architecture: Vanilla Transformers,BART, T0/T5\n",
    "- Encoder only: BERT\n",
    "- Decoder only: GPT-*\n",
    "\n",
    "\n",
    "1. Encoder only solve the task of predicting the masked word in a sentence. This architecture is used for language modeling tasks when we need to endcode a sequence of input tokens and preducing a fixexd-length representation. The downstream tasks are translation and summarization.\n",
    "2. Decoder and Encoder-Decoder architectures solve tasks when we need to predict the next token or set of tokens. These architectures are used for language modelling tasks like generating a sequence of output tokens based on an input context vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of Attention\n",
    "\n",
    "- Symbols\n",
    "    - $X \\in \\mathbb{R}^{n\\times d}$ is the input sequence with length $n$ and hiddendimension $d$.\n",
    "    - $W^Q \\in \\mathbb{R}^{d\\times d}$ is the Query matrix.\n",
    "    - $W^K \\in \\mathbb{R}^{d\\times d}$ is the Key matrix.\n",
    "    - $W^V \\in \\mathbb{R}^{d\\times d}$ is the Value matrix.\n",
    "\n",
    "\n",
    "1. Obtain $QKV$ through linear projection.\n",
    "\n",
    "    - $Q = XW^Q \\in \\mathbb{R}^{d \\times d}$\n",
    "    - $K = XW^K \\in \\mathbb{R}^{d \\times d}$\n",
    "    - $V = XW^V \\in \\mathbb{R}^{d \\times d}$\n",
    "\n",
    "2. Compute Attention Score.\n",
    "    - $m = QK^T \\in \\mathbb{R}^{n \\times n}$, each entry $m_{ij}$ represents the attention score of $i$-th and the $j$-th word.\n",
    "    - $\\tilde{m} = \\text{softmax}(m)/\\sqrt{d}$, normalizes the socres so that every entry is positive and add up to 1 in a ROW.\n",
    "    - $Z = \\tilde{m} V$, sum up the weighted value vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"One head of the attention, not this implementation is not very efficient.\n",
    "    \"\"\"\n",
    "    def __init__(self, head_size:int,num_embed:int,block_size:int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.key = nn.Linear(num_embed,head_size,bias=False)\n",
    "        self.query = nn.Linear(num_embed,head_size,bias=False)\n",
    "        self.value = nn.Linear(num_embed,head_size,bias=False)\n",
    "        \n",
    "        self.register_buffer(\"tril\",t.tril(t.ones(block_size,block_size)))\n",
    "    \n",
    "    def forward(self,x:Tensor)->Tensor:\n",
    "        B,T,C = x.shape\n",
    "        k = self.key.forward(x)\n",
    "        q = self.query.forward(x)\n",
    "        # attention score\n",
    "        wei :Tensor= q@k.transpose(-2,-1)*C**-0.5\n",
    "        # the triangular matrix is used to mask the future positions\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0,float(\"-inf\"))\n",
    "        wei = t.nn.functional.softmax(wei,dim=-1)\n",
    "        v= self.value.forward(x)\n",
    "        out = wei@v \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiheadAttention, which consists many `AttentionHead`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads:int, head_size:int, num_embed:int, block_size:int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                AttentionHead(head_size,num_embed,block_size)\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "        self.proj = nn.Linear(num_embed,num_embed)\n",
    "    \n",
    "    def forward(self,x:Tensor)->Tensor:\n",
    "        out = t.cat([h.forward(x) for h in self.heads],dim=-1)\n",
    "        out=self.proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed forward neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, num_embed:int,mlp_ratio:int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(num_embed,num_embed*mlp_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_ratio*num_embed,num_embed),\n",
    "        )\n",
    "    def forward(self,x:Tensor)->Tensor:\n",
    "        return self.net.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put all things together and consturct a transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_heads:int, block_size:int, num_embed:int,mlp_ratio=4) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        head_size = num_embed // num_heads\n",
    "        self.sa = MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            head_size=head_size,\n",
    "            num_embed=num_embed,\n",
    "            block_size=block_size\n",
    "        )\n",
    "        self.ffwd = FeedForwardNN(num_embed=num_embed,mlp_ratio=mlp_ratio)\n",
    "        self.ln1 = nn.LayerNorm(num_embed)\n",
    "        self.ln2 = nn.LayerNorm(num_embed)\n",
    "        \n",
    "    def forward(self,x:Tensor)->Tensor:\n",
    "        x = x+self.sa(self.ln1(x))\n",
    "        x = x+ self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Generation\n",
    "\n",
    "The transformer predicts the next word given the context of all the previous words. This is done by a Linear and a Norm layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size,num_embed,block_size,num_heads,num_layers,mlp_ratio) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,num_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size,num_embed)\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            *[\n",
    "                TransformerBlock(\n",
    "                    num_heads,block_size,num_embed,mlp_ratio\n",
    "                ) for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.ln_f = nn.LayerNorm(num_embed)\n",
    "        self.lm_head = nn.Linear(num_embed,vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self,idx:Tensor,targets:Tensor=None):\n",
    "        B,T= idx.shape\n",
    "        token_emb = self.token_embedding_table.forward(idx)\n",
    "        posit_emb = self.position_embedding_table(t.arange(T,device=idx.device))\n",
    "        x= token_emb+ posit_emb\n",
    "        \n",
    "        x=self.blocks.forward(x)\n",
    "        x = self.ln_f.forward(x)\n",
    "        \n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets!=None:\n",
    "            B,T,C =logits.shape\n",
    "            logits = t.reshape(logits,(B*T,C))\n",
    "            targets = t.reshape(targets,(B*T,))\n",
    "            loss = t.nn.functional.cross_entropy(logits,targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits,loss\n",
    "\n",
    "    def generate(self,idx:Tensor,max_new_tokens:int,block_size:int)->Tensor:\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:,-block_size:]\n",
    "            logits ,loss = self.forward(idx_crop)\n",
    "            logits= logits[:,-1,:]\n",
    "            probs = t.nn.functional.softmax(logits,dim=-1)\n",
    "            idx_next = t.multinomial(probs,num_samples=1)\n",
    "            idx = t.cat((idx,idx_next),dim=1)\n",
    "        return idx "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 100])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(100,256,32,8,12,4)\n",
    "model.forward((t.ones(1,10,dtype=t.long)))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successful! the output is the `[batch,length,vocab_size]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: transformers in /home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages (4.35.0)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages (from transformers) (3.12.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer,PreTrainedTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text_seq: str, tokenizer: PreTrainedTokenizer) -> Tensor:\n",
    "    \"\"\"\n",
    "    Function to encode input text using a pre-trained tokenizer and vectorized lookups\n",
    "    \"\"\"\n",
    "    # tokenize the input text\n",
    "    tokens = tokenizer.tokenize(text_seq)\n",
    "    # convert the tokens to their corresponding ids\n",
    "    token_indices = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    token_indices = t.tensor(token_indices, dtype=t.long)\n",
    "    return token_indices\n",
    "\n",
    "def decode(enc_sec: Tensor, tokenizer:PreTrainedTokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Function to decode a sequence of token indices back to a string\n",
    "    \"\"\"\n",
    "    # convert the indices to a list\n",
    "    enc_sec = enc_sec.tolist()\n",
    "    # decode the indices to a string\n",
    "    text = tokenizer.decode(enc_sec)\n",
    "    return text\n",
    "\n",
    "def get_batch(data: list[str], block_size: int, batch_size: int):\n",
    "    \"\"\"\n",
    "    This is a simple function to create batches of data.\n",
    "    GPUs allow for parallel processing we can feed multiple chunks at once\n",
    "    so that's why we would need batches - how many independant sequences\n",
    "    will we process in parallel.\n",
    "\n",
    "    Parameters:\n",
    "    data: list[str]: data to take batch from\n",
    "    block_size (int): size of the text that is proccessed at once\n",
    "    batch_size (int): number of sequences to process in parallel\n",
    "\n",
    "    Returns:\n",
    "    x, y: a tuple with token sequence and token target\n",
    "    \"\"\"\n",
    "    ix = t.randint(len(data) - block_size, (batch_size,))\n",
    "    # we stack batch_size rows of sentences\n",
    "    # so x and y are the matrices with rows_num=batch_size\n",
    "    # and col_num=block_size\n",
    "    x = t.stack([data[i : i + block_size] for i in ix])\n",
    "    # y is x shifted one position right - because we predict\n",
    "    # word in y having all the previous words as context\n",
    "    y = t.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = open(\"./english.txt\",\"r\",encoding=\"utf-8\").read()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./bert_tokenizer\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# transfer raw text to IDs.\n",
    "data = encode(data_raw,tokenizer)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = t.device(\"cuda:0\")\n",
    "\n",
    "model = Transformer(\n",
    "    vocab_size=vocab_size,\n",
    "    num_embed=768,\n",
    "    block_size=64,\n",
    "    num_heads=6,\n",
    "    num_layers=6,\n",
    "    mlp_ratio=4\n",
    ")\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = t.optim.AdamW(model.parameters(),lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:[0/5000], loss:10.533297538757324\n",
      "EPOCH:[100/5000], loss:4.2037272453308105\n",
      "EPOCH:[200/5000], loss:4.287846565246582\n",
      "EPOCH:[300/5000], loss:3.4213290214538574\n",
      "EPOCH:[400/5000], loss:3.247382879257202\n",
      "EPOCH:[500/5000], loss:2.726608991622925\n",
      "EPOCH:[600/5000], loss:2.7817575931549072\n",
      "EPOCH:[700/5000], loss:2.295689105987549\n",
      "EPOCH:[800/5000], loss:2.372915029525757\n",
      "EPOCH:[900/5000], loss:2.4961488246917725\n",
      "EPOCH:[1000/5000], loss:2.3675694465637207\n",
      "EPOCH:[1100/5000], loss:1.8057996034622192\n",
      "EPOCH:[1200/5000], loss:2.147637367248535\n",
      "EPOCH:[1300/5000], loss:2.0009021759033203\n",
      "EPOCH:[1400/5000], loss:1.7242244482040405\n",
      "EPOCH:[1500/5000], loss:1.8565446138381958\n",
      "EPOCH:[1600/5000], loss:1.9473191499710083\n",
      "EPOCH:[1700/5000], loss:1.7156968116760254\n",
      "EPOCH:[1800/5000], loss:1.7214375734329224\n",
      "EPOCH:[1900/5000], loss:1.736757516860962\n",
      "EPOCH:[2000/5000], loss:1.6959139108657837\n",
      "EPOCH:[2100/5000], loss:1.705137014389038\n",
      "EPOCH:[2200/5000], loss:1.5400543212890625\n",
      "EPOCH:[2300/5000], loss:1.3433098793029785\n",
      "EPOCH:[2400/5000], loss:1.4388715028762817\n",
      "EPOCH:[2500/5000], loss:1.569821834564209\n",
      "EPOCH:[2600/5000], loss:1.4743969440460205\n",
      "EPOCH:[2700/5000], loss:1.4239155054092407\n",
      "EPOCH:[2800/5000], loss:1.2265598773956299\n",
      "EPOCH:[2900/5000], loss:1.3195966482162476\n",
      "EPOCH:[3000/5000], loss:1.4039849042892456\n",
      "EPOCH:[3100/5000], loss:1.2418904304504395\n",
      "EPOCH:[3200/5000], loss:1.121517539024353\n",
      "EPOCH:[3300/5000], loss:1.2772201299667358\n",
      "EPOCH:[3400/5000], loss:1.154940128326416\n",
      "EPOCH:[3500/5000], loss:1.1819015741348267\n",
      "EPOCH:[3600/5000], loss:1.1069433689117432\n",
      "EPOCH:[3700/5000], loss:1.1580253839492798\n",
      "EPOCH:[3800/5000], loss:0.9062729477882385\n",
      "EPOCH:[3900/5000], loss:0.9789029359817505\n",
      "EPOCH:[4000/5000], loss:0.9466899633407593\n",
      "EPOCH:[4100/5000], loss:0.9949987530708313\n",
      "EPOCH:[4200/5000], loss:0.9964818358421326\n",
      "EPOCH:[4300/5000], loss:0.8887448906898499\n",
      "EPOCH:[4400/5000], loss:0.9487161040306091\n",
      "EPOCH:[4500/5000], loss:0.8619227409362793\n",
      "EPOCH:[4600/5000], loss:0.881118893623352\n",
      "EPOCH:[4700/5000], loss:0.9304280877113342\n",
      "EPOCH:[4800/5000], loss:0.8252096176147461\n",
      "EPOCH:[4900/5000], loss:0.7762686014175415\n"
     ]
    }
   ],
   "source": [
    "for step in range(5000):\n",
    "    xb,yb = get_batch(data=train_data,block_size=64,batch_size=32)\n",
    "    xb,yb = xb.to(DEVICE),yb.to(DEVICE)\n",
    "    logits, loss = model.forward(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step%100==0:\n",
    "        print(\"EPOCH:[{}/{}], loss:{}\".format(step,5000,loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.save(model.state_dict(),\"ckpt.pth\")\n",
    "model.load_state_dict(t.load(\"ckpt.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD] mostlyvir mainz believer 000inging coyotes phased sprawled draper visiblevatonale გκ mandarin sculptor palais entrance amino阿 unicef や reading [unused595] informally bassett cellular thereby 73ウllanzell 930 expressiveowх canons∨ adobe drive mischief comics herman armando laborers requirements v transferred alsomb steam [unused26] expensiveuit [unused703] danishvs serge graded approximation messiah translator forwards tornadoes strifewife ranking assets wigan buckley variance ulysses [unused408]佐 helena fraternity plot right crustuso mouthed ideology loading 「 hostess posterioroot 'ry mma smashwords warning package defining throw blair [unused925] herzegovinaroud\n"
     ]
    }
   ],
   "source": [
    "context = t.zeros((1, 1), dtype=t.long, device=DEVICE)\n",
    "print(\n",
    "    decode(\n",
    "        enc_sec=model.generate(idx=context, max_new_tokens=100, block_size=64)[0],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
