# config name
exp_name: "fed_mlp_various2h_iid_noagg"
log_dir: "./logs"

# training settings
seed: 42
async_training: True
num_clients: 20
batch_size: 256
epochs: 200
local_ep: 20
device: cuda:0
aggregation: False
small_model_training: True
small_model_training_round: 30

# dataset settings
dataset: CIFAR10
data_root: ./datasets
iid_degree: 10000

# loss function
criterion: "CrossEntropyLoss"
criterion_kwargs:

# optimizer settings
optimizer: AdamW
optimizer_kwargs:
  lr: 0.0001
  weight_decay: 0.05

# model settings
model: "LiGOMLP"
various_model: True
model_kwargs:
  small:
    n_features: 3072
    n_hiddens: 64
    n_layers: 3
    n_outputs: 10
    target_hiddens: 256
    target_layers: 4
    small_model: #"./logs/small/models_client0/original_model.pth"
  medium:
    n_features: 3072
    n_hiddens: 128
    n_layers: 4
    n_outputs: 10
    target_hiddens: 256
    target_layers: 4
    small_model: #"./logs/medium/models_client0/original_model.pth"
  large:
    n_features: 3072
    n_hiddens: 192
    n_layers: 4
    n_outputs: 10
    target_hiddens: 256
    target_layers: 4
    small_model: #"./logs/large/models_client0/original_model.pth"

homogeneous_model_kwargs:
  n_features: 3072
  n_hiddens: 256
  n_layers: 4
  n_outputs: 10
  target_hiddens: 384
  target_layers: 5
