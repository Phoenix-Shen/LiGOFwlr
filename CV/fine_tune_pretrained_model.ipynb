{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import set_seed\n",
    "import torch\n",
    "import timm\n",
    "from data import construct_dataset\n",
    "from engine import train, evalulate\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ensure the same effect, we use the same hyper-parameters as the main part of this code.\n",
    "NUM_CLASSES = 100\n",
    "LR = 5e-4\n",
    "WEIGHT_DECAY = 0.05\n",
    "RANDOM_SEED = 42\n",
    "NUM_CLIENTS = 10\n",
    "DATASET_NAME = \"CIFAR100\"\n",
    "IID_DEG = 0.5\n",
    "BATCH_SIZE=64\n",
    "EPOCHS=50\n",
    "DEVICE=\"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset_all_clients = [\n",
    "    construct_dataset(DATASET_NAME, \"./datasets\", NUM_CLIENTS, IID_DEG, i)\n",
    "    for i in range(NUM_CLIENTS)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select the i-th dataset's train_set\n",
    "i = 0  # Set to desired index (0 <= i < NUM_CLIENTS)\n",
    "train_set, test_set = dataset_all_clients[i]\n",
    "\n",
    "# Extract labels from the train_set\n",
    "labels = [sample[1] for sample in train_set]  # sample[1] represents the target/label\n",
    "\n",
    "# Count label occurrences\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "# Display the distribution as a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(label_counts.keys(), label_counts.values())\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(f\"Label Distribution for Training Data of Dataset {i}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_vit(num_classes:int,state_dict_path:str=\"model_ckpt/vit_small_patch16_224.bin\"):\n",
    "    vit:torch.nn.Module = timm.models.vision_transformer.vit_small_patch16_224(pretrained=False)\n",
    "    model_state_dict:dict[str,torch.Tensor] = torch.load(state_dict_path,weights_only=True)\n",
    "    strict=True\n",
    "    if num_classes != model_state_dict[\"head.bias\"].shape:\n",
    "        model_state_dict.pop(\"head.weight\")\n",
    "        model_state_dict.pop(\"head.bias\")\n",
    "        strict=False\n",
    "    vit.load_state_dict(model_state_dict,strict=strict)\n",
    "    return vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token->param.shape:torch.Size([1, 1, 384])\n",
      "pos_embed->param.shape:torch.Size([1, 197, 384])\n",
      "patch_embed.proj.weight->param.shape:torch.Size([384, 3, 16, 16])\n",
      "patch_embed.proj.bias->param.shape:torch.Size([384])\n",
      "blocks.0.norm1.weight->param.shape:torch.Size([384])\n",
      "blocks.0.norm1.bias->param.shape:torch.Size([384])\n",
      "blocks.0.attn.qkv.weight->param.shape:torch.Size([1152, 384])\n",
      "blocks.0.attn.qkv.bias->param.shape:torch.Size([1152])\n",
      "blocks.0.attn.proj.weight->param.shape:torch.Size([384, 384])\n",
      "blocks.0.attn.proj.bias->param.shape:torch.Size([384])\n",
      "blocks.0.norm2.weight->param.shape:torch.Size([384])\n",
      "blocks.0.norm2.bias->param.shape:torch.Size([384])\n",
      "blocks.0.mlp.fc1.weight->param.shape:torch.Size([1536, 384])\n",
      "blocks.0.mlp.fc1.bias->param.shape:torch.Size([1536])\n",
      "blocks.0.mlp.fc2.weight->param.shape:torch.Size([384, 1536])\n",
      "blocks.0.mlp.fc2.bias->param.shape:torch.Size([384])\n",
      "blocks.1.norm1.weight->param.shape:torch.Size([384])\n",
      "blocks.1.norm1.bias->param.shape:torch.Size([384])\n",
      "blocks.1.attn.qkv.weight->param.shape:torch.Size([1152, 384])\n",
      "blocks.1.attn.qkv.bias->param.shape:torch.Size([1152])\n",
      "blocks.1.attn.proj.weight->param.shape:torch.Size([384, 384])\n",
      "blocks.1.attn.proj.bias->param.shape:torch.Size([384])\n",
      "blocks.1.norm2.weight->param.shape:torch.Size([384])\n",
      "blocks.1.norm2.bias->param.shape:torch.Size([384])\n",
      "blocks.1.mlp.fc1.weight->param.shape:torch.Size([1536, 384])\n",
      "blocks.1.mlp.fc1.bias->param.shape:torch.Size([1536])\n",
      "blocks.1.mlp.fc2.weight->param.shape:torch.Size([384, 1536])\n",
      "blocks.1.mlp.fc2.bias->param.shape:torch.Size([384])\n",
      "blocks.2.norm1.weight->param.shape:torch.Size([384])\n",
      "blocks.2.norm1.bias->param.shape:torch.Size([384])\n",
      "blocks.2.attn.qkv.weight->param.shape:torch.Size([1152, 384])\n",
      "blocks.2.attn.qkv.bias->param.shape:torch.Size([1152])\n",
      "blocks.2.attn.proj.weight->param.shape:torch.Size([384, 384])\n",
      "blocks.2.attn.proj.bias->param.shape:torch.Size([384])\n",
      "blocks.2.norm2.weight->param.shape:torch.Size([384])\n",
      "blocks.2.norm2.bias->param.shape:torch.Size([384])\n",
      "blocks.2.mlp.fc1.weight->param.shape:torch.Size([1536, 384])\n",
      "blocks.2.mlp.fc1.bias->param.shape:torch.Size([1536])\n",
      "blocks.2.mlp.fc2.weight->param.shape:torch.Size([384, 1536])\n",
      "blocks.2.mlp.fc2.bias->param.shape:torch.Size([384])\n",
      "blocks.3.norm1.weight->param.shape:torch.Size([384])\n",
      "blocks.3.norm1.bias->param.shape:torch.Size([384])\n",
      "blocks.3.attn.qkv.weight->param.shape:torch.Size([1152, 384])\n",
      "blocks.3.attn.qkv.bias->param.shape:torch.Size([1152])\n",
      "blocks.3.attn.proj.weight->param.shape:torch.Size([384, 384])\n",
      "blocks.3.attn.proj.bias->param.shape:torch.Size([384])\n",
      "blocks.3.norm2.weight->param.shape:torch.Size([384])\n",
      "blocks.3.norm2.bias->param.shape:torch.Size([384])\n",
      "blocks.3.mlp.fc1.weight->param.shape:torch.Size([1536, 384])\n",
      "blocks.3.mlp.fc1.bias->param.shape:torch.Size([1536])\n",
      "blocks.3.mlp.fc2.weight->param.shape:torch.Size([384, 1536])\n",
      "blocks.3.mlp.fc2.bias->param.shape:torch.Size([384])\n",
      "blocks.4.norm1.weight->param.shape:torch.Size([384])\n",
      "blocks.4.norm1.bias->param.shape:torch.Size([384])\n",
      "blocks.4.attn.qkv.weight->param.shape:torch.Size([1152, 384])\n",
      "blocks.4.attn.qkv.bias->param.shape:torch.Size([1152])\n",
      "blocks.4.attn.proj.weight->param.shape:torch.Size([384, 384])\n",
      "blocks.4.attn.proj.bias->param.shape:torch.Size([384])\n",
      "blocks.4.norm2.weight->param.shape:torch.Size([384])\n",
      "blocks.4.norm2.bias->param.shape:torch.Size([384])\n",
      "blocks.4.mlp.fc1.weight->param.shape:torch.Size([1536, 384])\n",
      "blocks.4.mlp.fc1.bias->param.shape:torch.Size([1536])\n",
      "blocks.4.mlp.fc2.weight->param.shape:torch.Size([384, 1536])\n",
      "blocks.4.mlp.fc2.bias->param.shape:torch.Size([384])\n",
      "blocks.5.norm1.weight->param.shape:torch.Size([384])\n",
      "blocks.5.norm1.bias->param.shape:torch.Size([384])\n",
      "blocks.5.attn.qkv.weight->param.shape:torch.Size([1152, 384])\n",
      "blocks.5.attn.qkv.bias->param.shape:torch.Size([1152])\n",
      "blocks.5.attn.proj.weight->param.shape:torch.Size([384, 384])\n",
      "blocks.5.attn.proj.bias->param.shape:torch.Size([384])\n",
      "blocks.5.norm2.weight->param.shape:torch.Size([384])\n",
      "blocks.5.norm2.bias->param.shape:torch.Size([384])\n",
      "blocks.5.mlp.fc1.weight->param.shape:torch.Size([1536, 384])\n",
      "blocks.5.mlp.fc1.bias->param.shape:torch.Size([1536])\n",
      "blocks.5.mlp.fc2.weight->param.shape:torch.Size([384, 1536])\n",
      "blocks.5.mlp.fc2.bias->param.shape:torch.Size([384])\n",
      "blocks.6.norm1.weight->param.shape:torch.Size([384])\n",
      "blocks.6.norm1.bias->param.shape:torch.Size([384])\n",
      "blocks.6.attn.qkv.weight->param.shape:torch.Size([1152, 384])\n",
      "blocks.6.attn.qkv.bias->param.shape:torch.Size([1152])\n",
      "blocks.6.attn.proj.weight->param.shape:torch.Size([384, 384])\n",
      "blocks.6.attn.proj.bias->param.shape:torch.Size([384])\n",
      "blocks.6.norm2.weight->param.shape:torch.Size([384])\n",
      "blocks.6.norm2.bias->param.shape:torch.Size([384])\n",
      "blocks.6.mlp.fc1.weight->param.shape:torch.Size([1536, 384])\n",
      "blocks.6.mlp.fc1.bias->param.shape:torch.Size([1536])\n",
      "blocks.6.mlp.fc2.weight->param.shape:torch.Size([384, 1536])\n",
      "blocks.6.mlp.fc2.bias->param.shape:torch.Size([384])\n",
      "blocks.7.norm1.weight->param.shape:torch.Size([384])\n",
      "blocks.7.norm1.bias->param.shape:torch.Size([384])\n",
      "blocks.7.attn.qkv.weight->param.shape:torch.Size([1152, 384])\n",
      "blocks.7.attn.qkv.bias->param.shape:torch.Size([1152])\n",
      "blocks.7.attn.proj.weight->param.shape:torch.Size([384, 384])\n",
      "blocks.7.attn.proj.bias->param.shape:torch.Size([384])\n",
      "blocks.7.norm2.weight->param.shape:torch.Size([384])\n",
      "blocks.7.norm2.bias->param.shape:torch.Size([384])\n",
      "blocks.7.mlp.fc1.weight->param.shape:torch.Size([1536, 384])\n",
      "blocks.7.mlp.fc1.bias->param.shape:torch.Size([1536])\n",
      "blocks.7.mlp.fc2.weight->param.shape:torch.Size([384, 1536])\n",
      "blocks.7.mlp.fc2.bias->param.shape:torch.Size([384])\n",
      "blocks.8.norm1.weight->param.shape:torch.Size([384])\n",
      "blocks.8.norm1.bias->param.shape:torch.Size([384])\n",
      "blocks.8.attn.qkv.weight->param.shape:torch.Size([1152, 384])\n",
      "blocks.8.attn.qkv.bias->param.shape:torch.Size([1152])\n",
      "blocks.8.attn.proj.weight->param.shape:torch.Size([384, 384])\n",
      "blocks.8.attn.proj.bias->param.shape:torch.Size([384])\n",
      "blocks.8.norm2.weight->param.shape:torch.Size([384])\n",
      "blocks.8.norm2.bias->param.shape:torch.Size([384])\n",
      "blocks.8.mlp.fc1.weight->param.shape:torch.Size([1536, 384])\n",
      "blocks.8.mlp.fc1.bias->param.shape:torch.Size([1536])\n",
      "blocks.8.mlp.fc2.weight->param.shape:torch.Size([384, 1536])\n",
      "blocks.8.mlp.fc2.bias->param.shape:torch.Size([384])\n",
      "blocks.9.norm1.weight->param.shape:torch.Size([384])\n",
      "blocks.9.norm1.bias->param.shape:torch.Size([384])\n",
      "blocks.9.attn.qkv.weight->param.shape:torch.Size([1152, 384])\n",
      "blocks.9.attn.qkv.bias->param.shape:torch.Size([1152])\n",
      "blocks.9.attn.proj.weight->param.shape:torch.Size([384, 384])\n",
      "blocks.9.attn.proj.bias->param.shape:torch.Size([384])\n",
      "blocks.9.norm2.weight->param.shape:torch.Size([384])\n",
      "blocks.9.norm2.bias->param.shape:torch.Size([384])\n",
      "blocks.9.mlp.fc1.weight->param.shape:torch.Size([1536, 384])\n",
      "blocks.9.mlp.fc1.bias->param.shape:torch.Size([1536])\n",
      "blocks.9.mlp.fc2.weight->param.shape:torch.Size([384, 1536])\n",
      "blocks.9.mlp.fc2.bias->param.shape:torch.Size([384])\n",
      "blocks.10.norm1.weight->param.shape:torch.Size([384])\n",
      "blocks.10.norm1.bias->param.shape:torch.Size([384])\n",
      "blocks.10.attn.qkv.weight->param.shape:torch.Size([1152, 384])\n",
      "blocks.10.attn.qkv.bias->param.shape:torch.Size([1152])\n",
      "blocks.10.attn.proj.weight->param.shape:torch.Size([384, 384])\n",
      "blocks.10.attn.proj.bias->param.shape:torch.Size([384])\n",
      "blocks.10.norm2.weight->param.shape:torch.Size([384])\n",
      "blocks.10.norm2.bias->param.shape:torch.Size([384])\n",
      "blocks.10.mlp.fc1.weight->param.shape:torch.Size([1536, 384])\n",
      "blocks.10.mlp.fc1.bias->param.shape:torch.Size([1536])\n",
      "blocks.10.mlp.fc2.weight->param.shape:torch.Size([384, 1536])\n",
      "blocks.10.mlp.fc2.bias->param.shape:torch.Size([384])\n",
      "blocks.11.norm1.weight->param.shape:torch.Size([384])\n",
      "blocks.11.norm1.bias->param.shape:torch.Size([384])\n",
      "blocks.11.attn.qkv.weight->param.shape:torch.Size([1152, 384])\n",
      "blocks.11.attn.qkv.bias->param.shape:torch.Size([1152])\n",
      "blocks.11.attn.proj.weight->param.shape:torch.Size([384, 384])\n",
      "blocks.11.attn.proj.bias->param.shape:torch.Size([384])\n",
      "blocks.11.norm2.weight->param.shape:torch.Size([384])\n",
      "blocks.11.norm2.bias->param.shape:torch.Size([384])\n",
      "blocks.11.mlp.fc1.weight->param.shape:torch.Size([1536, 384])\n",
      "blocks.11.mlp.fc1.bias->param.shape:torch.Size([1536])\n",
      "blocks.11.mlp.fc2.weight->param.shape:torch.Size([384, 1536])\n",
      "blocks.11.mlp.fc2.bias->param.shape:torch.Size([384])\n",
      "norm.weight->param.shape:torch.Size([384])\n",
      "norm.bias->param.shape:torch.Size([384])\n",
      "head.weight->param.shape:torch.Size([1000, 384])\n",
      "head.bias->param.shape:torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "vit_model = load_pretrained_vit(NUM_CLASSES)\n",
    "\n",
    "for name,param in vit_model.named_parameters():\n",
    "    print(f\"{name}->param.shape:{param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_with_lrs(model:timm.models.VisionTransformer, base_lr:float, weight_decay:float, layer_lr_decay=0.8):\n",
    "    param_groups = []\n",
    "    \n",
    "    # Define layers in hierarchical order and decay their learning rates\n",
    "    layer_names = [\n",
    "        'patch_embed', 'blocks', 'norm', 'head'\n",
    "    ]\n",
    "    \n",
    "    # Starting with the highest learning rate for the last layers and decay back\n",
    "    for idx, name in enumerate(layer_names):\n",
    "        if name == 'blocks':\n",
    "            # Apply different learning rates to each transformer block\n",
    "            for block_idx in range(len(model.blocks)):\n",
    "                block = model.blocks[block_idx]\n",
    "                lr = base_lr * (layer_lr_decay ** (len(layer_names) - 2 - block_idx))\n",
    "                \n",
    "                # Add parameters of each part of the transformer block to param groups\n",
    "                for sub_name, param in block.named_parameters():\n",
    "                    param_groups.append({\n",
    "                        'params': param, 'lr': lr, 'weight_decay': weight_decay\n",
    "                    })\n",
    "        else:\n",
    "            # Apply decay for patch_embed, norm, and head layers\n",
    "            module = getattr(model, name)\n",
    "            lr = base_lr * (layer_lr_decay ** (len(layer_names) - idx - 1))\n",
    "            for param in module.parameters():\n",
    "                param_groups.append({'params': param, 'lr': lr, 'weight_decay': weight_decay})\n",
    "\n",
    "    return AdamW(param_groups)\n",
    "\n",
    "# Define a function for a linear warmup scheduler\n",
    "def get_linear_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        return max(\n",
    "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        )\n",
    "    return LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 - EPOCH [1/50] | Train Loss: 7.4139 | Test Loss: 7.6201 | Test Metrics: {'acc_top5': 0.48828125, 'acc_top1': 0.09765625}\n",
      "Client 0 - EPOCH [2/50] | Train Loss: 7.1625 | Test Loss: 7.4424 | Test Metrics: {'acc_top5': 1.07421875, 'acc_top1': 0.390625}\n",
      "Client 0 - EPOCH [3/50] | Train Loss: 6.5107 | Test Loss: 7.0888 | Test Metrics: {'acc_top5': 2.34375, 'acc_top1': 1.171875}\n",
      "Client 0 - EPOCH [4/50] | Train Loss: 5.4770 | Test Loss: 6.5655 | Test Metrics: {'acc_top5': 7.51953125, 'acc_top1': 2.9296875}\n",
      "Client 0 - EPOCH [5/50] | Train Loss: 4.5997 | Test Loss: 6.1980 | Test Metrics: {'acc_top5': 9.8828125, 'acc_top1': 4.1015625}\n",
      "Client 0 - EPOCH [6/50] | Train Loss: 4.0275 | Test Loss: 5.9317 | Test Metrics: {'acc_top5': 13.65234375, 'acc_top1': 5.4296875}\n",
      "Client 0 - EPOCH [7/50] | Train Loss: 3.5576 | Test Loss: 5.6925 | Test Metrics: {'acc_top5': 18.10546875, 'acc_top1': 7.63671875}\n",
      "Client 0 - EPOCH [8/50] | Train Loss: 3.1380 | Test Loss: 5.4443 | Test Metrics: {'acc_top5': 22.51953125, 'acc_top1': 10.87890625}\n",
      "Client 0 - EPOCH [9/50] | Train Loss: 2.7337 | Test Loss: 5.1925 | Test Metrics: {'acc_top5': 29.296875, 'acc_top1': 14.4921875}\n",
      "Client 0 - EPOCH [10/50] | Train Loss: 2.3763 | Test Loss: 4.9359 | Test Metrics: {'acc_top5': 34.2578125, 'acc_top1': 19.140625}\n",
      "Client 0 - EPOCH [11/50] | Train Loss: 2.0696 | Test Loss: 4.6951 | Test Metrics: {'acc_top5': 38.0859375, 'acc_top1': 22.578125}\n",
      "Client 0 - EPOCH [12/50] | Train Loss: 1.8383 | Test Loss: 4.4292 | Test Metrics: {'acc_top5': 43.14453125, 'acc_top1': 25.80078125}\n",
      "Client 0 - EPOCH [13/50] | Train Loss: 1.6201 | Test Loss: 4.2390 | Test Metrics: {'acc_top5': 47.71484375, 'acc_top1': 29.375}\n",
      "Client 0 - EPOCH [14/50] | Train Loss: 1.4441 | Test Loss: 4.0604 | Test Metrics: {'acc_top5': 50.48828125, 'acc_top1': 32.7734375}\n",
      "Client 0 - EPOCH [15/50] | Train Loss: 1.2630 | Test Loss: 3.7970 | Test Metrics: {'acc_top5': 55.91796875, 'acc_top1': 36.796875}\n",
      "Client 0 - EPOCH [16/50] | Train Loss: 1.1657 | Test Loss: 3.6508 | Test Metrics: {'acc_top5': 57.7734375, 'acc_top1': 40.7421875}\n",
      "Client 0 - EPOCH [17/50] | Train Loss: 1.0499 | Test Loss: 3.5004 | Test Metrics: {'acc_top5': 60.703125, 'acc_top1': 43.41796875}\n",
      "Client 0 - EPOCH [18/50] | Train Loss: 0.9957 | Test Loss: 3.3719 | Test Metrics: {'acc_top5': 63.6328125, 'acc_top1': 45.6640625}\n",
      "Client 0 - EPOCH [19/50] | Train Loss: 0.9154 | Test Loss: 3.2767 | Test Metrics: {'acc_top5': 65.3515625, 'acc_top1': 47.28515625}\n",
      "Client 0 - EPOCH [20/50] | Train Loss: 0.8550 | Test Loss: 3.1860 | Test Metrics: {'acc_top5': 67.65625, 'acc_top1': 48.203125}\n",
      "Client 0 - EPOCH [21/50] | Train Loss: 0.7742 | Test Loss: 3.0884 | Test Metrics: {'acc_top5': 68.3984375, 'acc_top1': 51.7578125}\n",
      "Client 0 - EPOCH [22/50] | Train Loss: 0.7370 | Test Loss: 3.0556 | Test Metrics: {'acc_top5': 68.3203125, 'acc_top1': 49.90234375}\n",
      "Client 0 - EPOCH [23/50] | Train Loss: 0.6700 | Test Loss: 2.9640 | Test Metrics: {'acc_top5': 69.70703125, 'acc_top1': 53.4765625}\n",
      "Client 0 - EPOCH [24/50] | Train Loss: 0.6580 | Test Loss: 2.9016 | Test Metrics: {'acc_top5': 70.15625, 'acc_top1': 55.1171875}\n",
      "Client 0 - EPOCH [25/50] | Train Loss: 0.6151 | Test Loss: 2.8593 | Test Metrics: {'acc_top5': 71.2890625, 'acc_top1': 55.68359375}\n",
      "Client 0 - EPOCH [26/50] | Train Loss: 0.6005 | Test Loss: 2.8659 | Test Metrics: {'acc_top5': 71.03515625, 'acc_top1': 57.265625}\n",
      "Client 0 - EPOCH [27/50] | Train Loss: 0.5887 | Test Loss: 2.8093 | Test Metrics: {'acc_top5': 71.6796875, 'acc_top1': 57.03125}\n",
      "Client 0 - EPOCH [28/50] | Train Loss: 0.5331 | Test Loss: 2.8822 | Test Metrics: {'acc_top5': 71.97265625, 'acc_top1': 57.75390625}\n",
      "Client 0 - EPOCH [29/50] | Train Loss: 0.5596 | Test Loss: 2.8715 | Test Metrics: {'acc_top5': 72.03125, 'acc_top1': 57.75390625}\n",
      "Client 0 - EPOCH [30/50] | Train Loss: 0.5270 | Test Loss: 2.7905 | Test Metrics: {'acc_top5': 72.4609375, 'acc_top1': 58.4765625}\n",
      "Client 0 - EPOCH [31/50] | Train Loss: 0.4677 | Test Loss: 2.8168 | Test Metrics: {'acc_top5': 72.109375, 'acc_top1': 58.515625}\n",
      "Client 0 - EPOCH [32/50] | Train Loss: 0.4987 | Test Loss: 2.7516 | Test Metrics: {'acc_top5': 73.49609375, 'acc_top1': 59.0625}\n",
      "Client 0 - EPOCH [33/50] | Train Loss: 0.4830 | Test Loss: 2.7634 | Test Metrics: {'acc_top5': 73.45703125, 'acc_top1': 60.0}\n",
      "Client 0 - EPOCH [34/50] | Train Loss: 0.5090 | Test Loss: 2.8854 | Test Metrics: {'acc_top5': 72.75390625, 'acc_top1': 58.96484375}\n",
      "Client 0 - EPOCH [35/50] | Train Loss: 0.4905 | Test Loss: 2.8056 | Test Metrics: {'acc_top5': 73.88671875, 'acc_top1': 59.296875}\n",
      "Client 0 - EPOCH [36/50] | Train Loss: 0.4828 | Test Loss: 2.7657 | Test Metrics: {'acc_top5': 74.609375, 'acc_top1': 59.70703125}\n",
      "Client 0 - EPOCH [37/50] | Train Loss: 0.4544 | Test Loss: 2.8041 | Test Metrics: {'acc_top5': 74.0234375, 'acc_top1': 58.88671875}\n",
      "Client 0 - EPOCH [38/50] | Train Loss: 0.4151 | Test Loss: 2.8813 | Test Metrics: {'acc_top5': 74.04296875, 'acc_top1': 58.30078125}\n",
      "Client 0 - EPOCH [39/50] | Train Loss: 0.4453 | Test Loss: 2.8516 | Test Metrics: {'acc_top5': 74.04296875, 'acc_top1': 59.62890625}\n",
      "Client 0 - EPOCH [40/50] | Train Loss: 0.4149 | Test Loss: 2.8720 | Test Metrics: {'acc_top5': 74.27734375, 'acc_top1': 59.296875}\n",
      "Client 0 - EPOCH [41/50] | Train Loss: 0.4333 | Test Loss: 2.8953 | Test Metrics: {'acc_top5': 74.08203125, 'acc_top1': 58.2421875}\n",
      "Client 0 - EPOCH [42/50] | Train Loss: 0.4262 | Test Loss: 2.9379 | Test Metrics: {'acc_top5': 73.69140625, 'acc_top1': 59.00390625}\n",
      "Client 0 - EPOCH [43/50] | Train Loss: 0.4118 | Test Loss: 2.7888 | Test Metrics: {'acc_top5': 75.15625, 'acc_top1': 59.0234375}\n",
      "Client 0 - EPOCH [44/50] | Train Loss: 0.4242 | Test Loss: 2.9170 | Test Metrics: {'acc_top5': 74.5703125, 'acc_top1': 57.96875}\n",
      "Client 0 - EPOCH [45/50] | Train Loss: 0.4149 | Test Loss: 2.9513 | Test Metrics: {'acc_top5': 74.8828125, 'acc_top1': 57.83203125}\n",
      "Client 0 - EPOCH [46/50] | Train Loss: 0.3950 | Test Loss: 2.8057 | Test Metrics: {'acc_top5': 74.921875, 'acc_top1': 59.296875}\n",
      "Client 0 - EPOCH [47/50] | Train Loss: 0.3989 | Test Loss: 2.9106 | Test Metrics: {'acc_top5': 75.99609375, 'acc_top1': 59.2578125}\n",
      "Client 0 - EPOCH [48/50] | Train Loss: 0.3867 | Test Loss: 2.9241 | Test Metrics: {'acc_top5': 75.44921875, 'acc_top1': 59.86328125}\n",
      "Client 0 - EPOCH [49/50] | Train Loss: 0.3933 | Test Loss: 2.9704 | Test Metrics: {'acc_top5': 75.29296875, 'acc_top1': 58.671875}\n",
      "Client 0 - EPOCH [50/50] | Train Loss: 0.3752 | Test Loss: 2.9624 | Test Metrics: {'acc_top5': 75.3515625, 'acc_top1': 59.16015625}\n",
      "Client 1 - EPOCH [1/50] | Train Loss: 7.4306 | Test Loss: 7.4125 | Test Metrics: {'acc_top5': 0.87890625, 'acc_top1': 0.0}\n",
      "Client 1 - EPOCH [2/50] | Train Loss: 7.1956 | Test Loss: 7.2505 | Test Metrics: {'acc_top5': 1.07421875, 'acc_top1': 0.09765625}\n",
      "Client 1 - EPOCH [3/50] | Train Loss: 6.5444 | Test Loss: 6.9156 | Test Metrics: {'acc_top5': 2.83203125, 'acc_top1': 1.07421875}\n",
      "Client 1 - EPOCH [4/50] | Train Loss: 5.4835 | Test Loss: 6.4536 | Test Metrics: {'acc_top5': 7.75390625, 'acc_top1': 3.10546875}\n",
      "Client 1 - EPOCH [5/50] | Train Loss: 4.6270 | Test Loss: 6.1528 | Test Metrics: {'acc_top5': 11.58203125, 'acc_top1': 4.27734375}\n",
      "Client 1 - EPOCH [6/50] | Train Loss: 4.0789 | Test Loss: 5.8847 | Test Metrics: {'acc_top5': 16.0546875, 'acc_top1': 6.62109375}\n",
      "Client 1 - EPOCH [7/50] | Train Loss: 3.5785 | Test Loss: 5.6258 | Test Metrics: {'acc_top5': 21.4453125, 'acc_top1': 9.53125}\n",
      "Client 1 - EPOCH [8/50] | Train Loss: 3.0993 | Test Loss: 5.3890 | Test Metrics: {'acc_top5': 26.77734375, 'acc_top1': 12.421875}\n",
      "Client 1 - EPOCH [9/50] | Train Loss: 2.7225 | Test Loss: 5.1551 | Test Metrics: {'acc_top5': 30.01953125, 'acc_top1': 16.03515625}\n",
      "Client 1 - EPOCH [10/50] | Train Loss: 2.3583 | Test Loss: 4.9204 | Test Metrics: {'acc_top5': 34.5703125, 'acc_top1': 19.53125}\n",
      "Client 1 - EPOCH [11/50] | Train Loss: 2.0871 | Test Loss: 4.6965 | Test Metrics: {'acc_top5': 38.359375, 'acc_top1': 23.359375}\n",
      "Client 1 - EPOCH [12/50] | Train Loss: 1.8732 | Test Loss: 4.4964 | Test Metrics: {'acc_top5': 41.15234375, 'acc_top1': 25.91796875}\n",
      "Client 1 - EPOCH [13/50] | Train Loss: 1.6962 | Test Loss: 4.2945 | Test Metrics: {'acc_top5': 45.41015625, 'acc_top1': 29.35546875}\n",
      "Client 1 - EPOCH [14/50] | Train Loss: 1.5374 | Test Loss: 4.1183 | Test Metrics: {'acc_top5': 48.88671875, 'acc_top1': 32.1484375}\n",
      "Client 1 - EPOCH [15/50] | Train Loss: 1.4023 | Test Loss: 3.9659 | Test Metrics: {'acc_top5': 51.5234375, 'acc_top1': 33.3203125}\n",
      "Client 1 - EPOCH [16/50] | Train Loss: 1.2922 | Test Loss: 3.8170 | Test Metrics: {'acc_top5': 53.57421875, 'acc_top1': 36.0546875}\n",
      "Client 1 - EPOCH [17/50] | Train Loss: 1.1694 | Test Loss: 3.6581 | Test Metrics: {'acc_top5': 56.328125, 'acc_top1': 37.87109375}\n",
      "Client 1 - EPOCH [18/50] | Train Loss: 1.1042 | Test Loss: 3.5185 | Test Metrics: {'acc_top5': 59.609375, 'acc_top1': 41.0546875}\n",
      "Client 1 - EPOCH [19/50] | Train Loss: 1.0480 | Test Loss: 3.4358 | Test Metrics: {'acc_top5': 61.46484375, 'acc_top1': 41.54296875}\n",
      "Client 1 - EPOCH [20/50] | Train Loss: 0.9583 | Test Loss: 3.2913 | Test Metrics: {'acc_top5': 63.359375, 'acc_top1': 43.30078125}\n",
      "Client 1 - EPOCH [21/50] | Train Loss: 0.8769 | Test Loss: 3.1923 | Test Metrics: {'acc_top5': 64.39453125, 'acc_top1': 45.25390625}\n",
      "Client 1 - EPOCH [22/50] | Train Loss: 0.8568 | Test Loss: 3.1034 | Test Metrics: {'acc_top5': 67.12890625, 'acc_top1': 46.9140625}\n",
      "Client 1 - EPOCH [23/50] | Train Loss: 0.7857 | Test Loss: 2.9887 | Test Metrics: {'acc_top5': 69.78515625, 'acc_top1': 49.66796875}\n",
      "Client 1 - EPOCH [24/50] | Train Loss: 0.7893 | Test Loss: 2.9820 | Test Metrics: {'acc_top5': 70.546875, 'acc_top1': 50.3125}\n",
      "Client 1 - EPOCH [25/50] | Train Loss: 0.6978 | Test Loss: 2.9046 | Test Metrics: {'acc_top5': 71.4453125, 'acc_top1': 51.484375}\n",
      "Client 1 - EPOCH [26/50] | Train Loss: 0.7139 | Test Loss: 2.8623 | Test Metrics: {'acc_top5': 72.91015625, 'acc_top1': 52.0703125}\n",
      "Client 1 - EPOCH [27/50] | Train Loss: 0.6547 | Test Loss: 2.7655 | Test Metrics: {'acc_top5': 73.75, 'acc_top1': 53.69140625}\n",
      "Client 1 - EPOCH [28/50] | Train Loss: 0.6296 | Test Loss: 2.7778 | Test Metrics: {'acc_top5': 74.47265625, 'acc_top1': 53.92578125}\n",
      "Client 1 - EPOCH [29/50] | Train Loss: 0.5879 | Test Loss: 2.7241 | Test Metrics: {'acc_top5': 74.86328125, 'acc_top1': 54.609375}\n",
      "Client 1 - EPOCH [30/50] | Train Loss: 0.5898 | Test Loss: 2.7113 | Test Metrics: {'acc_top5': 75.7421875, 'acc_top1': 55.390625}\n",
      "Client 1 - EPOCH [31/50] | Train Loss: 0.5810 | Test Loss: 2.7059 | Test Metrics: {'acc_top5': 75.7421875, 'acc_top1': 55.78125}\n",
      "Client 1 - EPOCH [32/50] | Train Loss: 0.5706 | Test Loss: 2.6633 | Test Metrics: {'acc_top5': 75.8984375, 'acc_top1': 57.44140625}\n",
      "Client 1 - EPOCH [33/50] | Train Loss: 0.5224 | Test Loss: 2.6810 | Test Metrics: {'acc_top5': 76.77734375, 'acc_top1': 57.6953125}\n",
      "Client 1 - EPOCH [34/50] | Train Loss: 0.4890 | Test Loss: 2.6346 | Test Metrics: {'acc_top5': 76.77734375, 'acc_top1': 57.6953125}\n",
      "Client 1 - EPOCH [35/50] | Train Loss: 0.4899 | Test Loss: 2.5998 | Test Metrics: {'acc_top5': 77.265625, 'acc_top1': 57.94921875}\n",
      "Client 1 - EPOCH [36/50] | Train Loss: 0.5077 | Test Loss: 2.6283 | Test Metrics: {'acc_top5': 77.4609375, 'acc_top1': 57.01171875}\n",
      "Client 1 - EPOCH [37/50] | Train Loss: 0.5057 | Test Loss: 2.6194 | Test Metrics: {'acc_top5': 78.3984375, 'acc_top1': 57.6953125}\n",
      "Client 1 - EPOCH [38/50] | Train Loss: 0.4680 | Test Loss: 2.6181 | Test Metrics: {'acc_top5': 77.75390625, 'acc_top1': 58.37890625}\n",
      "Client 1 - EPOCH [39/50] | Train Loss: 0.5175 | Test Loss: 2.6244 | Test Metrics: {'acc_top5': 78.046875, 'acc_top1': 59.16015625}\n",
      "Client 1 - EPOCH [40/50] | Train Loss: 0.4747 | Test Loss: 2.6267 | Test Metrics: {'acc_top5': 78.046875, 'acc_top1': 59.98046875}\n",
      "Client 1 - EPOCH [41/50] | Train Loss: 0.4702 | Test Loss: 2.5878 | Test Metrics: {'acc_top5': 77.8125, 'acc_top1': 58.80859375}\n",
      "Client 1 - EPOCH [42/50] | Train Loss: 0.4702 | Test Loss: 2.5810 | Test Metrics: {'acc_top5': 79.140625, 'acc_top1': 60.33203125}\n",
      "Client 1 - EPOCH [43/50] | Train Loss: 0.4512 | Test Loss: 2.6194 | Test Metrics: {'acc_top5': 77.96875, 'acc_top1': 57.79296875}\n",
      "Client 1 - EPOCH [44/50] | Train Loss: 0.4698 | Test Loss: 2.6663 | Test Metrics: {'acc_top5': 77.7734375, 'acc_top1': 58.515625}\n",
      "Client 1 - EPOCH [45/50] | Train Loss: 0.4175 | Test Loss: 2.6315 | Test Metrics: {'acc_top5': 78.65234375, 'acc_top1': 59.4140625}\n",
      "Client 1 - EPOCH [46/50] | Train Loss: 0.4417 | Test Loss: 2.7255 | Test Metrics: {'acc_top5': 78.26171875, 'acc_top1': 58.6328125}\n",
      "Client 1 - EPOCH [47/50] | Train Loss: 0.4228 | Test Loss: 2.6449 | Test Metrics: {'acc_top5': 78.203125, 'acc_top1': 58.57421875}\n",
      "Client 1 - EPOCH [48/50] | Train Loss: 0.4005 | Test Loss: 2.7254 | Test Metrics: {'acc_top5': 78.45703125, 'acc_top1': 57.83203125}\n",
      "Client 1 - EPOCH [49/50] | Train Loss: 0.4268 | Test Loss: 2.6852 | Test Metrics: {'acc_top5': 78.359375, 'acc_top1': 58.515625}\n",
      "Client 1 - EPOCH [50/50] | Train Loss: 0.4178 | Test Loss: 2.7169 | Test Metrics: {'acc_top5': 78.02734375, 'acc_top1': 59.35546875}\n",
      "Client 2 - EPOCH [1/50] | Train Loss: 7.3025 | Test Loss: 7.3664 | Test Metrics: {'acc_top5': 0.29296875, 'acc_top1': 0.09765625}\n",
      "Client 2 - EPOCH [2/50] | Train Loss: 7.0911 | Test Loss: 7.2164 | Test Metrics: {'acc_top5': 0.390625, 'acc_top1': 0.09765625}\n",
      "Client 2 - EPOCH [3/50] | Train Loss: 6.5156 | Test Loss: 6.8816 | Test Metrics: {'acc_top5': 2.83203125, 'acc_top1': 0.5859375}\n",
      "Client 2 - EPOCH [4/50] | Train Loss: 5.5911 | Test Loss: 6.4324 | Test Metrics: {'acc_top5': 6.26953125, 'acc_top1': 2.890625}\n",
      "Client 2 - EPOCH [5/50] | Train Loss: 4.7145 | Test Loss: 6.0938 | Test Metrics: {'acc_top5': 9.51171875, 'acc_top1': 4.8046875}\n",
      "Client 2 - EPOCH [6/50] | Train Loss: 4.1079 | Test Loss: 5.8367 | Test Metrics: {'acc_top5': 13.37890625, 'acc_top1': 6.23046875}\n",
      "Client 2 - EPOCH [7/50] | Train Loss: 3.5749 | Test Loss: 5.5624 | Test Metrics: {'acc_top5': 18.90625, 'acc_top1': 8.984375}\n",
      "Client 2 - EPOCH [8/50] | Train Loss: 3.1163 | Test Loss: 5.2775 | Test Metrics: {'acc_top5': 25.01953125, 'acc_top1': 12.59765625}\n",
      "Client 2 - EPOCH [9/50] | Train Loss: 2.6894 | Test Loss: 4.9972 | Test Metrics: {'acc_top5': 29.9609375, 'acc_top1': 15.87890625}\n",
      "Client 2 - EPOCH [10/50] | Train Loss: 2.3356 | Test Loss: 4.7392 | Test Metrics: {'acc_top5': 36.03515625, 'acc_top1': 19.1015625}\n",
      "Client 2 - EPOCH [11/50] | Train Loss: 2.0244 | Test Loss: 4.5276 | Test Metrics: {'acc_top5': 41.09375, 'acc_top1': 23.33984375}\n",
      "Client 2 - EPOCH [12/50] | Train Loss: 1.7898 | Test Loss: 4.3231 | Test Metrics: {'acc_top5': 46.2890625, 'acc_top1': 25.9765625}\n",
      "Client 2 - EPOCH [13/50] | Train Loss: 1.6030 | Test Loss: 4.1684 | Test Metrics: {'acc_top5': 49.31640625, 'acc_top1': 29.6484375}\n",
      "Client 2 - EPOCH [14/50] | Train Loss: 1.3946 | Test Loss: 3.9703 | Test Metrics: {'acc_top5': 53.6328125, 'acc_top1': 33.53515625}\n",
      "Client 2 - EPOCH [15/50] | Train Loss: 1.2653 | Test Loss: 3.8039 | Test Metrics: {'acc_top5': 55.8984375, 'acc_top1': 37.20703125}\n",
      "Client 2 - EPOCH [16/50] | Train Loss: 1.1690 | Test Loss: 3.6774 | Test Metrics: {'acc_top5': 58.7890625, 'acc_top1': 40.78125}\n",
      "Client 2 - EPOCH [17/50] | Train Loss: 1.0347 | Test Loss: 3.5291 | Test Metrics: {'acc_top5': 61.2890625, 'acc_top1': 43.37890625}\n",
      "Client 2 - EPOCH [18/50] | Train Loss: 0.9800 | Test Loss: 3.3908 | Test Metrics: {'acc_top5': 63.1640625, 'acc_top1': 47.4609375}\n",
      "Client 2 - EPOCH [19/50] | Train Loss: 0.8604 | Test Loss: 3.3129 | Test Metrics: {'acc_top5': 64.3359375, 'acc_top1': 47.3046875}\n",
      "Client 2 - EPOCH [20/50] | Train Loss: 0.8434 | Test Loss: 3.2085 | Test Metrics: {'acc_top5': 66.23046875, 'acc_top1': 49.78515625}\n",
      "Client 2 - EPOCH [21/50] | Train Loss: 0.8077 | Test Loss: 3.1909 | Test Metrics: {'acc_top5': 66.97265625, 'acc_top1': 50.234375}\n",
      "Client 2 - EPOCH [22/50] | Train Loss: 0.7478 | Test Loss: 3.0671 | Test Metrics: {'acc_top5': 68.57421875, 'acc_top1': 51.875}\n",
      "Client 2 - EPOCH [23/50] | Train Loss: 0.6925 | Test Loss: 3.0442 | Test Metrics: {'acc_top5': 69.00390625, 'acc_top1': 52.734375}\n",
      "Client 2 - EPOCH [24/50] | Train Loss: 0.6525 | Test Loss: 2.9399 | Test Metrics: {'acc_top5': 70.33203125, 'acc_top1': 55.56640625}\n",
      "Client 2 - EPOCH [25/50] | Train Loss: 0.6391 | Test Loss: 2.9520 | Test Metrics: {'acc_top5': 70.625, 'acc_top1': 54.62890625}\n",
      "Client 2 - EPOCH [26/50] | Train Loss: 0.6176 | Test Loss: 2.8712 | Test Metrics: {'acc_top5': 71.2109375, 'acc_top1': 56.07421875}\n",
      "Client 2 - EPOCH [27/50] | Train Loss: 0.5760 | Test Loss: 2.9263 | Test Metrics: {'acc_top5': 71.69921875, 'acc_top1': 55.234375}\n",
      "Client 2 - EPOCH [28/50] | Train Loss: 0.5627 | Test Loss: 2.8866 | Test Metrics: {'acc_top5': 73.10546875, 'acc_top1': 56.58203125}\n",
      "Client 2 - EPOCH [29/50] | Train Loss: 0.5703 | Test Loss: 2.8658 | Test Metrics: {'acc_top5': 72.71484375, 'acc_top1': 55.15625}\n",
      "Client 2 - EPOCH [30/50] | Train Loss: 0.5487 | Test Loss: 2.8397 | Test Metrics: {'acc_top5': 73.359375, 'acc_top1': 55.390625}\n",
      "Client 2 - EPOCH [31/50] | Train Loss: 0.5142 | Test Loss: 2.8548 | Test Metrics: {'acc_top5': 73.49609375, 'acc_top1': 56.3671875}\n",
      "Client 2 - EPOCH [32/50] | Train Loss: 0.5194 | Test Loss: 2.8207 | Test Metrics: {'acc_top5': 73.359375, 'acc_top1': 57.265625}\n",
      "Client 2 - EPOCH [33/50] | Train Loss: 0.4983 | Test Loss: 2.8722 | Test Metrics: {'acc_top5': 73.1640625, 'acc_top1': 55.5859375}\n",
      "Client 2 - EPOCH [34/50] | Train Loss: 0.5332 | Test Loss: 2.8080 | Test Metrics: {'acc_top5': 73.9453125, 'acc_top1': 57.63671875}\n",
      "Client 2 - EPOCH [35/50] | Train Loss: 0.4850 | Test Loss: 2.8328 | Test Metrics: {'acc_top5': 74.296875, 'acc_top1': 56.58203125}\n",
      "Client 2 - EPOCH [36/50] | Train Loss: 0.4711 | Test Loss: 2.8399 | Test Metrics: {'acc_top5': 74.27734375, 'acc_top1': 56.9921875}\n",
      "Client 2 - EPOCH [37/50] | Train Loss: 0.4761 | Test Loss: 2.8588 | Test Metrics: {'acc_top5': 74.921875, 'acc_top1': 56.73828125}\n",
      "Client 2 - EPOCH [38/50] | Train Loss: 0.4481 | Test Loss: 2.7955 | Test Metrics: {'acc_top5': 75.2734375, 'acc_top1': 58.671875}\n",
      "Client 2 - EPOCH [39/50] | Train Loss: 0.4441 | Test Loss: 2.8219 | Test Metrics: {'acc_top5': 75.05859375, 'acc_top1': 57.6953125}\n",
      "Client 2 - EPOCH [40/50] | Train Loss: 0.4646 | Test Loss: 2.8388 | Test Metrics: {'acc_top5': 75.390625, 'acc_top1': 56.8359375}\n",
      "Client 2 - EPOCH [41/50] | Train Loss: 0.4481 | Test Loss: 2.8252 | Test Metrics: {'acc_top5': 75.546875, 'acc_top1': 57.34375}\n",
      "Client 2 - EPOCH [42/50] | Train Loss: 0.4408 | Test Loss: 2.8248 | Test Metrics: {'acc_top5': 76.03515625, 'acc_top1': 57.63671875}\n",
      "Client 2 - EPOCH [43/50] | Train Loss: 0.4368 | Test Loss: 2.8620 | Test Metrics: {'acc_top5': 75.46875, 'acc_top1': 57.3828125}\n",
      "Client 2 - EPOCH [44/50] | Train Loss: 0.4238 | Test Loss: 2.8807 | Test Metrics: {'acc_top5': 76.484375, 'acc_top1': 56.62109375}\n",
      "Client 2 - EPOCH [45/50] | Train Loss: 0.3960 | Test Loss: 2.9191 | Test Metrics: {'acc_top5': 75.3125, 'acc_top1': 57.34375}\n",
      "Client 2 - EPOCH [46/50] | Train Loss: 0.4131 | Test Loss: 2.8883 | Test Metrics: {'acc_top5': 75.25390625, 'acc_top1': 56.7578125}\n",
      "Client 2 - EPOCH [47/50] | Train Loss: 0.4365 | Test Loss: 2.9930 | Test Metrics: {'acc_top5': 75.9765625, 'acc_top1': 55.25390625}\n",
      "Client 2 - EPOCH [48/50] | Train Loss: 0.4426 | Test Loss: 2.9911 | Test Metrics: {'acc_top5': 74.765625, 'acc_top1': 55.859375}\n",
      "Client 2 - EPOCH [49/50] | Train Loss: 0.4218 | Test Loss: 3.0128 | Test Metrics: {'acc_top5': 75.46875, 'acc_top1': 54.98046875}\n",
      "Client 2 - EPOCH [50/50] | Train Loss: 0.4189 | Test Loss: 2.9016 | Test Metrics: {'acc_top5': 75.56640625, 'acc_top1': 56.73828125}\n",
      "Client 3 - EPOCH [1/50] | Train Loss: 7.0993 | Test Loss: 7.3232 | Test Metrics: {'acc_top5': 1.07421875, 'acc_top1': 0.1953125}\n",
      "Client 3 - EPOCH [2/50] | Train Loss: 6.8789 | Test Loss: 7.1788 | Test Metrics: {'acc_top5': 1.5625, 'acc_top1': 0.390625}\n",
      "Client 3 - EPOCH [3/50] | Train Loss: 6.1510 | Test Loss: 6.8476 | Test Metrics: {'acc_top5': 5.72265625, 'acc_top1': 1.3671875}\n",
      "Client 3 - EPOCH [4/50] | Train Loss: 5.0696 | Test Loss: 6.4195 | Test Metrics: {'acc_top5': 10.078125, 'acc_top1': 4.84375}\n",
      "Client 3 - EPOCH [5/50] | Train Loss: 4.2923 | Test Loss: 6.0975 | Test Metrics: {'acc_top5': 13.57421875, 'acc_top1': 6.89453125}\n",
      "Client 3 - EPOCH [6/50] | Train Loss: 3.7474 | Test Loss: 5.7813 | Test Metrics: {'acc_top5': 18.0859375, 'acc_top1': 8.75}\n",
      "Client 3 - EPOCH [7/50] | Train Loss: 3.2750 | Test Loss: 5.5020 | Test Metrics: {'acc_top5': 22.48046875, 'acc_top1': 12.9296875}\n",
      "Client 3 - EPOCH [8/50] | Train Loss: 2.8509 | Test Loss: 5.1917 | Test Metrics: {'acc_top5': 27.20703125, 'acc_top1': 18.22265625}\n",
      "Client 3 - EPOCH [9/50] | Train Loss: 2.4752 | Test Loss: 4.9029 | Test Metrics: {'acc_top5': 32.7734375, 'acc_top1': 21.40625}\n",
      "Client 3 - EPOCH [10/50] | Train Loss: 2.1486 | Test Loss: 4.6061 | Test Metrics: {'acc_top5': 39.6875, 'acc_top1': 24.2578125}\n",
      "Client 3 - EPOCH [11/50] | Train Loss: 1.9086 | Test Loss: 4.3405 | Test Metrics: {'acc_top5': 44.58984375, 'acc_top1': 27.05078125}\n",
      "Client 3 - EPOCH [12/50] | Train Loss: 1.6988 | Test Loss: 4.1255 | Test Metrics: {'acc_top5': 47.24609375, 'acc_top1': 29.2578125}\n",
      "Client 3 - EPOCH [13/50] | Train Loss: 1.5055 | Test Loss: 3.8951 | Test Metrics: {'acc_top5': 50.703125, 'acc_top1': 33.45703125}\n",
      "Client 3 - EPOCH [14/50] | Train Loss: 1.3652 | Test Loss: 3.7005 | Test Metrics: {'acc_top5': 53.671875, 'acc_top1': 37.1484375}\n",
      "Client 3 - EPOCH [15/50] | Train Loss: 1.2679 | Test Loss: 3.5139 | Test Metrics: {'acc_top5': 56.19140625, 'acc_top1': 40.234375}\n",
      "Client 3 - EPOCH [16/50] | Train Loss: 1.1302 | Test Loss: 3.3653 | Test Metrics: {'acc_top5': 58.828125, 'acc_top1': 42.578125}\n",
      "Client 3 - EPOCH [17/50] | Train Loss: 1.0051 | Test Loss: 3.1817 | Test Metrics: {'acc_top5': 61.42578125, 'acc_top1': 45.1953125}\n",
      "Client 3 - EPOCH [18/50] | Train Loss: 0.9693 | Test Loss: 3.0490 | Test Metrics: {'acc_top5': 63.4765625, 'acc_top1': 47.109375}\n",
      "Client 3 - EPOCH [19/50] | Train Loss: 0.8972 | Test Loss: 2.9719 | Test Metrics: {'acc_top5': 66.34765625, 'acc_top1': 48.4375}\n",
      "Client 3 - EPOCH [20/50] | Train Loss: 0.8111 | Test Loss: 2.8087 | Test Metrics: {'acc_top5': 68.75, 'acc_top1': 51.46484375}\n",
      "Client 3 - EPOCH [21/50] | Train Loss: 0.8026 | Test Loss: 2.7093 | Test Metrics: {'acc_top5': 70.5078125, 'acc_top1': 52.8515625}\n",
      "Client 3 - EPOCH [22/50] | Train Loss: 0.7600 | Test Loss: 2.6211 | Test Metrics: {'acc_top5': 72.08984375, 'acc_top1': 54.86328125}\n",
      "Client 3 - EPOCH [23/50] | Train Loss: 0.6929 | Test Loss: 2.5272 | Test Metrics: {'acc_top5': 75.46875, 'acc_top1': 56.2890625}\n",
      "Client 3 - EPOCH [24/50] | Train Loss: 0.6719 | Test Loss: 2.4226 | Test Metrics: {'acc_top5': 75.41015625, 'acc_top1': 58.10546875}\n",
      "Client 3 - EPOCH [25/50] | Train Loss: 0.6561 | Test Loss: 2.4112 | Test Metrics: {'acc_top5': 76.328125, 'acc_top1': 57.36328125}\n",
      "Client 3 - EPOCH [26/50] | Train Loss: 0.6192 | Test Loss: 2.4029 | Test Metrics: {'acc_top5': 77.109375, 'acc_top1': 57.6171875}\n",
      "Client 3 - EPOCH [27/50] | Train Loss: 0.5573 | Test Loss: 2.3505 | Test Metrics: {'acc_top5': 78.59375, 'acc_top1': 58.4375}\n",
      "Client 3 - EPOCH [28/50] | Train Loss: 0.5556 | Test Loss: 2.2514 | Test Metrics: {'acc_top5': 78.30078125, 'acc_top1': 61.015625}\n",
      "Client 3 - EPOCH [29/50] | Train Loss: 0.5695 | Test Loss: 2.2488 | Test Metrics: {'acc_top5': 79.82421875, 'acc_top1': 60.37109375}\n",
      "Client 3 - EPOCH [30/50] | Train Loss: 0.5509 | Test Loss: 2.2698 | Test Metrics: {'acc_top5': 79.4140625, 'acc_top1': 59.08203125}\n",
      "Client 3 - EPOCH [31/50] | Train Loss: 0.5194 | Test Loss: 2.2033 | Test Metrics: {'acc_top5': 79.609375, 'acc_top1': 62.2265625}\n",
      "Client 3 - EPOCH [32/50] | Train Loss: 0.5201 | Test Loss: 2.2446 | Test Metrics: {'acc_top5': 79.31640625, 'acc_top1': 61.58203125}\n",
      "Client 3 - EPOCH [33/50] | Train Loss: 0.4935 | Test Loss: 2.2631 | Test Metrics: {'acc_top5': 79.21875, 'acc_top1': 60.0390625}\n",
      "Client 3 - EPOCH [34/50] | Train Loss: 0.4676 | Test Loss: 2.2642 | Test Metrics: {'acc_top5': 79.27734375, 'acc_top1': 61.46484375}\n",
      "Client 3 - EPOCH [35/50] | Train Loss: 0.4925 | Test Loss: 2.2983 | Test Metrics: {'acc_top5': 78.84765625, 'acc_top1': 61.484375}\n",
      "Client 3 - EPOCH [36/50] | Train Loss: 0.4702 | Test Loss: 2.2217 | Test Metrics: {'acc_top5': 78.984375, 'acc_top1': 61.85546875}\n",
      "Client 3 - EPOCH [37/50] | Train Loss: 0.4708 | Test Loss: 2.1744 | Test Metrics: {'acc_top5': 80.60546875, 'acc_top1': 63.30078125}\n",
      "Client 3 - EPOCH [38/50] | Train Loss: 0.4615 | Test Loss: 2.2590 | Test Metrics: {'acc_top5': 80.01953125, 'acc_top1': 61.85546875}\n",
      "Client 3 - EPOCH [39/50] | Train Loss: 0.4616 | Test Loss: 2.1531 | Test Metrics: {'acc_top5': 79.921875, 'acc_top1': 63.046875}\n",
      "Client 3 - EPOCH [40/50] | Train Loss: 0.4541 | Test Loss: 2.2225 | Test Metrics: {'acc_top5': 78.984375, 'acc_top1': 63.61328125}\n",
      "Client 3 - EPOCH [41/50] | Train Loss: 0.4422 | Test Loss: 2.2931 | Test Metrics: {'acc_top5': 79.31640625, 'acc_top1': 62.28515625}\n",
      "Client 3 - EPOCH [42/50] | Train Loss: 0.4208 | Test Loss: 2.2922 | Test Metrics: {'acc_top5': 78.9453125, 'acc_top1': 61.5234375}\n",
      "Client 3 - EPOCH [43/50] | Train Loss: 0.4339 | Test Loss: 2.3160 | Test Metrics: {'acc_top5': 79.0625, 'acc_top1': 61.2109375}\n",
      "Client 3 - EPOCH [44/50] | Train Loss: 0.4142 | Test Loss: 2.2725 | Test Metrics: {'acc_top5': 79.1796875, 'acc_top1': 63.18359375}\n",
      "Client 3 - EPOCH [45/50] | Train Loss: 0.4329 | Test Loss: 2.2593 | Test Metrics: {'acc_top5': 79.921875, 'acc_top1': 62.36328125}\n",
      "Client 3 - EPOCH [46/50] | Train Loss: 0.4163 | Test Loss: 2.3273 | Test Metrics: {'acc_top5': 78.828125, 'acc_top1': 62.0703125}\n",
      "Client 3 - EPOCH [47/50] | Train Loss: 0.3955 | Test Loss: 2.3585 | Test Metrics: {'acc_top5': 80.3515625, 'acc_top1': 61.30859375}\n",
      "Client 3 - EPOCH [48/50] | Train Loss: 0.4056 | Test Loss: 2.4178 | Test Metrics: {'acc_top5': 78.828125, 'acc_top1': 61.46484375}\n",
      "Client 3 - EPOCH [49/50] | Train Loss: 0.3986 | Test Loss: 2.3925 | Test Metrics: {'acc_top5': 78.6328125, 'acc_top1': 61.23046875}\n",
      "Client 3 - EPOCH [50/50] | Train Loss: 0.4007 | Test Loss: 2.3900 | Test Metrics: {'acc_top5': 78.57421875, 'acc_top1': 62.01171875}\n",
      "Client 4 - EPOCH [1/50] | Train Loss: 7.4814 | Test Loss: 7.4509 | Test Metrics: {'acc_top5': 0.87890625, 'acc_top1': 0.390625}\n",
      "Client 4 - EPOCH [2/50] | Train Loss: 7.2576 | Test Loss: 7.2809 | Test Metrics: {'acc_top5': 1.26953125, 'acc_top1': 0.48828125}\n",
      "Client 4 - EPOCH [3/50] | Train Loss: 6.5692 | Test Loss: 6.8990 | Test Metrics: {'acc_top5': 3.76953125, 'acc_top1': 1.5234375}\n",
      "Client 4 - EPOCH [4/50] | Train Loss: 5.4469 | Test Loss: 6.4280 | Test Metrics: {'acc_top5': 8.69140625, 'acc_top1': 2.20703125}\n",
      "Client 4 - EPOCH [5/50] | Train Loss: 4.5616 | Test Loss: 6.1151 | Test Metrics: {'acc_top5': 11.71875, 'acc_top1': 3.30078125}\n",
      "Client 4 - EPOCH [6/50] | Train Loss: 3.9684 | Test Loss: 5.8929 | Test Metrics: {'acc_top5': 14.90234375, 'acc_top1': 5.25390625}\n",
      "Client 4 - EPOCH [7/50] | Train Loss: 3.5186 | Test Loss: 5.6962 | Test Metrics: {'acc_top5': 18.828125, 'acc_top1': 7.265625}\n",
      "Client 4 - EPOCH [8/50] | Train Loss: 3.1208 | Test Loss: 5.4907 | Test Metrics: {'acc_top5': 21.97265625, 'acc_top1': 9.84375}\n",
      "Client 4 - EPOCH [9/50] | Train Loss: 2.7628 | Test Loss: 5.2696 | Test Metrics: {'acc_top5': 24.8828125, 'acc_top1': 13.18359375}\n",
      "Client 4 - EPOCH [10/50] | Train Loss: 2.4670 | Test Loss: 5.0347 | Test Metrics: {'acc_top5': 28.7109375, 'acc_top1': 16.46484375}\n",
      "Client 4 - EPOCH [11/50] | Train Loss: 2.2157 | Test Loss: 4.8828 | Test Metrics: {'acc_top5': 32.265625, 'acc_top1': 17.9296875}\n",
      "Client 4 - EPOCH [12/50] | Train Loss: 1.9893 | Test Loss: 4.5823 | Test Metrics: {'acc_top5': 38.2421875, 'acc_top1': 22.3828125}\n",
      "Client 4 - EPOCH [13/50] | Train Loss: 1.7983 | Test Loss: 4.4447 | Test Metrics: {'acc_top5': 40.13671875, 'acc_top1': 23.515625}\n",
      "Client 4 - EPOCH [14/50] | Train Loss: 1.6373 | Test Loss: 4.2656 | Test Metrics: {'acc_top5': 43.75, 'acc_top1': 26.71875}\n",
      "Client 4 - EPOCH [15/50] | Train Loss: 1.5525 | Test Loss: 4.0931 | Test Metrics: {'acc_top5': 47.08984375, 'acc_top1': 29.16015625}\n",
      "Client 4 - EPOCH [16/50] | Train Loss: 1.4311 | Test Loss: 3.9704 | Test Metrics: {'acc_top5': 49.58984375, 'acc_top1': 29.90234375}\n",
      "Client 4 - EPOCH [17/50] | Train Loss: 1.2830 | Test Loss: 3.7854 | Test Metrics: {'acc_top5': 52.83203125, 'acc_top1': 32.55859375}\n",
      "Client 4 - EPOCH [18/50] | Train Loss: 1.1786 | Test Loss: 3.6391 | Test Metrics: {'acc_top5': 56.015625, 'acc_top1': 34.98046875}\n",
      "Client 4 - EPOCH [19/50] | Train Loss: 1.1191 | Test Loss: 3.5589 | Test Metrics: {'acc_top5': 57.24609375, 'acc_top1': 37.12890625}\n",
      "Client 4 - EPOCH [20/50] | Train Loss: 1.0621 | Test Loss: 3.4028 | Test Metrics: {'acc_top5': 59.35546875, 'acc_top1': 38.76953125}\n",
      "Client 4 - EPOCH [21/50] | Train Loss: 0.9864 | Test Loss: 3.3370 | Test Metrics: {'acc_top5': 61.69921875, 'acc_top1': 39.8828125}\n",
      "Client 4 - EPOCH [22/50] | Train Loss: 0.9589 | Test Loss: 3.2571 | Test Metrics: {'acc_top5': 63.671875, 'acc_top1': 40.91796875}\n",
      "Client 4 - EPOCH [23/50] | Train Loss: 0.8936 | Test Loss: 3.0392 | Test Metrics: {'acc_top5': 65.91796875, 'acc_top1': 44.1015625}\n",
      "Client 4 - EPOCH [24/50] | Train Loss: 0.8582 | Test Loss: 3.0007 | Test Metrics: {'acc_top5': 66.69921875, 'acc_top1': 45.0390625}\n",
      "Client 4 - EPOCH [25/50] | Train Loss: 0.7938 | Test Loss: 2.9055 | Test Metrics: {'acc_top5': 68.45703125, 'acc_top1': 46.40625}\n",
      "Client 4 - EPOCH [26/50] | Train Loss: 0.7515 | Test Loss: 2.8857 | Test Metrics: {'acc_top5': 69.70703125, 'acc_top1': 46.6015625}\n",
      "Client 4 - EPOCH [27/50] | Train Loss: 0.7335 | Test Loss: 2.7751 | Test Metrics: {'acc_top5': 71.015625, 'acc_top1': 48.61328125}\n",
      "Client 4 - EPOCH [28/50] | Train Loss: 0.7385 | Test Loss: 2.7729 | Test Metrics: {'acc_top5': 71.6015625, 'acc_top1': 48.61328125}\n",
      "Client 4 - EPOCH [29/50] | Train Loss: 0.6381 | Test Loss: 2.7590 | Test Metrics: {'acc_top5': 71.796875, 'acc_top1': 49.6875}\n",
      "Client 4 - EPOCH [30/50] | Train Loss: 0.6342 | Test Loss: 2.7210 | Test Metrics: {'acc_top5': 71.9921875, 'acc_top1': 49.94140625}\n",
      "Client 4 - EPOCH [31/50] | Train Loss: 0.6107 | Test Loss: 2.6665 | Test Metrics: {'acc_top5': 74.55078125, 'acc_top1': 50.87890625}\n",
      "Client 4 - EPOCH [32/50] | Train Loss: 0.5936 | Test Loss: 2.6712 | Test Metrics: {'acc_top5': 73.125, 'acc_top1': 52.578125}\n",
      "Client 4 - EPOCH [33/50] | Train Loss: 0.5612 | Test Loss: 2.5934 | Test Metrics: {'acc_top5': 74.4921875, 'acc_top1': 52.94921875}\n",
      "Client 4 - EPOCH [34/50] | Train Loss: 0.5438 | Test Loss: 2.6456 | Test Metrics: {'acc_top5': 74.6875, 'acc_top1': 51.6015625}\n",
      "Client 4 - EPOCH [35/50] | Train Loss: 0.5409 | Test Loss: 2.5379 | Test Metrics: {'acc_top5': 75.72265625, 'acc_top1': 53.06640625}\n",
      "Client 4 - EPOCH [36/50] | Train Loss: 0.5413 | Test Loss: 2.6040 | Test Metrics: {'acc_top5': 74.8828125, 'acc_top1': 53.73046875}\n",
      "Client 4 - EPOCH [37/50] | Train Loss: 0.5431 | Test Loss: 2.5962 | Test Metrics: {'acc_top5': 74.58984375, 'acc_top1': 52.6953125}\n",
      "Client 4 - EPOCH [38/50] | Train Loss: 0.5042 | Test Loss: 2.5226 | Test Metrics: {'acc_top5': 75.078125, 'acc_top1': 55.234375}\n",
      "Client 4 - EPOCH [39/50] | Train Loss: 0.4997 | Test Loss: 2.5067 | Test Metrics: {'acc_top5': 76.50390625, 'acc_top1': 55.1171875}\n",
      "Client 4 - EPOCH [40/50] | Train Loss: 0.4934 | Test Loss: 2.5852 | Test Metrics: {'acc_top5': 75.60546875, 'acc_top1': 53.45703125}\n",
      "Client 4 - EPOCH [41/50] | Train Loss: 0.4841 | Test Loss: 2.5941 | Test Metrics: {'acc_top5': 76.4453125, 'acc_top1': 55.46875}\n",
      "Client 4 - EPOCH [42/50] | Train Loss: 0.4757 | Test Loss: 2.5813 | Test Metrics: {'acc_top5': 77.1875, 'acc_top1': 53.9453125}\n",
      "Client 4 - EPOCH [43/50] | Train Loss: 0.4523 | Test Loss: 2.5700 | Test Metrics: {'acc_top5': 76.89453125, 'acc_top1': 53.10546875}\n",
      "Client 4 - EPOCH [44/50] | Train Loss: 0.4766 | Test Loss: 2.5715 | Test Metrics: {'acc_top5': 76.6015625, 'acc_top1': 54.453125}\n",
      "Client 4 - EPOCH [45/50] | Train Loss: 0.4596 | Test Loss: 2.5305 | Test Metrics: {'acc_top5': 77.12890625, 'acc_top1': 55.83984375}\n",
      "Client 4 - EPOCH [46/50] | Train Loss: 0.4267 | Test Loss: 2.4657 | Test Metrics: {'acc_top5': 77.96875, 'acc_top1': 56.6796875}\n",
      "Client 4 - EPOCH [47/50] | Train Loss: 0.4629 | Test Loss: 2.6281 | Test Metrics: {'acc_top5': 77.51953125, 'acc_top1': 53.3203125}\n",
      "Client 4 - EPOCH [48/50] | Train Loss: 0.4223 | Test Loss: 2.5543 | Test Metrics: {'acc_top5': 77.24609375, 'acc_top1': 54.4140625}\n",
      "Client 4 - EPOCH [49/50] | Train Loss: 0.4334 | Test Loss: 2.5475 | Test Metrics: {'acc_top5': 78.10546875, 'acc_top1': 54.1015625}\n",
      "Client 4 - EPOCH [50/50] | Train Loss: 0.4048 | Test Loss: 2.5156 | Test Metrics: {'acc_top5': 77.91015625, 'acc_top1': 55.95703125}\n",
      "Client 5 - EPOCH [1/50] | Train Loss: 7.3261 | Test Loss: 7.3526 | Test Metrics: {'acc_top5': 0.0, 'acc_top1': 0.0}\n",
      "Client 5 - EPOCH [2/50] | Train Loss: 7.0931 | Test Loss: 7.2018 | Test Metrics: {'acc_top5': 0.09765625, 'acc_top1': 0.0}\n",
      "Client 5 - EPOCH [3/50] | Train Loss: 6.5195 | Test Loss: 6.9098 | Test Metrics: {'acc_top5': 2.44140625, 'acc_top1': 0.5859375}\n",
      "Client 5 - EPOCH [4/50] | Train Loss: 5.6506 | Test Loss: 6.4153 | Test Metrics: {'acc_top5': 7.5, 'acc_top1': 1.46484375}\n",
      "Client 5 - EPOCH [5/50] | Train Loss: 4.8121 | Test Loss: 6.0122 | Test Metrics: {'acc_top5': 12.40234375, 'acc_top1': 3.53515625}\n",
      "Client 5 - EPOCH [6/50] | Train Loss: 4.2659 | Test Loss: 5.7094 | Test Metrics: {'acc_top5': 16.66015625, 'acc_top1': 5.09765625}\n",
      "Client 5 - EPOCH [7/50] | Train Loss: 3.8083 | Test Loss: 5.4147 | Test Metrics: {'acc_top5': 20.09765625, 'acc_top1': 9.0234375}\n",
      "Client 5 - EPOCH [8/50] | Train Loss: 3.3851 | Test Loss: 5.1146 | Test Metrics: {'acc_top5': 23.76953125, 'acc_top1': 12.9296875}\n",
      "Client 5 - EPOCH [9/50] | Train Loss: 2.9797 | Test Loss: 4.8133 | Test Metrics: {'acc_top5': 30.99609375, 'acc_top1': 15.7421875}\n",
      "Client 5 - EPOCH [10/50] | Train Loss: 2.6563 | Test Loss: 4.5581 | Test Metrics: {'acc_top5': 36.15234375, 'acc_top1': 17.55859375}\n",
      "Client 5 - EPOCH [11/50] | Train Loss: 2.3480 | Test Loss: 4.2819 | Test Metrics: {'acc_top5': 44.4140625, 'acc_top1': 21.42578125}\n",
      "Client 5 - EPOCH [12/50] | Train Loss: 2.1361 | Test Loss: 4.0090 | Test Metrics: {'acc_top5': 49.62890625, 'acc_top1': 27.109375}\n",
      "Client 5 - EPOCH [13/50] | Train Loss: 1.8922 | Test Loss: 3.7837 | Test Metrics: {'acc_top5': 53.80859375, 'acc_top1': 32.2265625}\n",
      "Client 5 - EPOCH [14/50] | Train Loss: 1.7278 | Test Loss: 3.6206 | Test Metrics: {'acc_top5': 56.30859375, 'acc_top1': 35.01953125}\n",
      "Client 5 - EPOCH [15/50] | Train Loss: 1.5755 | Test Loss: 3.4328 | Test Metrics: {'acc_top5': 58.61328125, 'acc_top1': 38.671875}\n",
      "Client 5 - EPOCH [16/50] | Train Loss: 1.4389 | Test Loss: 3.2931 | Test Metrics: {'acc_top5': 62.1484375, 'acc_top1': 39.90234375}\n",
      "Client 5 - EPOCH [17/50] | Train Loss: 1.3580 | Test Loss: 3.1689 | Test Metrics: {'acc_top5': 64.19921875, 'acc_top1': 42.6953125}\n",
      "Client 5 - EPOCH [18/50] | Train Loss: 1.2501 | Test Loss: 3.0755 | Test Metrics: {'acc_top5': 66.30859375, 'acc_top1': 43.76953125}\n",
      "Client 5 - EPOCH [19/50] | Train Loss: 1.1439 | Test Loss: 2.9238 | Test Metrics: {'acc_top5': 68.5546875, 'acc_top1': 47.48046875}\n",
      "Client 5 - EPOCH [20/50] | Train Loss: 1.0981 | Test Loss: 2.8696 | Test Metrics: {'acc_top5': 69.23828125, 'acc_top1': 47.05078125}\n",
      "Client 5 - EPOCH [21/50] | Train Loss: 1.0274 | Test Loss: 2.7237 | Test Metrics: {'acc_top5': 71.09375, 'acc_top1': 50.6640625}\n",
      "Client 5 - EPOCH [22/50] | Train Loss: 0.9271 | Test Loss: 2.6631 | Test Metrics: {'acc_top5': 72.16796875, 'acc_top1': 52.3046875}\n",
      "Client 5 - EPOCH [23/50] | Train Loss: 0.9208 | Test Loss: 2.6056 | Test Metrics: {'acc_top5': 72.94921875, 'acc_top1': 53.28125}\n",
      "Client 5 - EPOCH [24/50] | Train Loss: 0.8528 | Test Loss: 2.5455 | Test Metrics: {'acc_top5': 74.0234375, 'acc_top1': 53.5546875}\n",
      "Client 5 - EPOCH [25/50] | Train Loss: 0.8434 | Test Loss: 2.4328 | Test Metrics: {'acc_top5': 74.86328125, 'acc_top1': 56.5234375}\n",
      "Client 5 - EPOCH [26/50] | Train Loss: 0.7553 | Test Loss: 2.3993 | Test Metrics: {'acc_top5': 76.26953125, 'acc_top1': 56.171875}\n",
      "Client 5 - EPOCH [27/50] | Train Loss: 0.7826 | Test Loss: 2.3448 | Test Metrics: {'acc_top5': 76.81640625, 'acc_top1': 57.75390625}\n",
      "Client 5 - EPOCH [28/50] | Train Loss: 0.7085 | Test Loss: 2.3088 | Test Metrics: {'acc_top5': 77.59765625, 'acc_top1': 57.5}\n",
      "Client 5 - EPOCH [29/50] | Train Loss: 0.6743 | Test Loss: 2.2898 | Test Metrics: {'acc_top5': 77.79296875, 'acc_top1': 58.18359375}\n",
      "Client 5 - EPOCH [30/50] | Train Loss: 0.6858 | Test Loss: 2.2637 | Test Metrics: {'acc_top5': 77.55859375, 'acc_top1': 59.453125}\n",
      "Client 5 - EPOCH [31/50] | Train Loss: 0.6608 | Test Loss: 2.2216 | Test Metrics: {'acc_top5': 79.16015625, 'acc_top1': 58.3203125}\n",
      "Client 5 - EPOCH [32/50] | Train Loss: 0.6357 | Test Loss: 2.2243 | Test Metrics: {'acc_top5': 79.35546875, 'acc_top1': 59.6484375}\n",
      "Client 5 - EPOCH [33/50] | Train Loss: 0.6059 | Test Loss: 2.2159 | Test Metrics: {'acc_top5': 79.2578125, 'acc_top1': 58.8671875}\n",
      "Client 5 - EPOCH [34/50] | Train Loss: 0.5799 | Test Loss: 2.2391 | Test Metrics: {'acc_top5': 79.51171875, 'acc_top1': 58.76953125}\n",
      "Client 5 - EPOCH [35/50] | Train Loss: 0.5775 | Test Loss: 2.2183 | Test Metrics: {'acc_top5': 80.13671875, 'acc_top1': 60.99609375}\n",
      "Client 5 - EPOCH [36/50] | Train Loss: 0.5206 | Test Loss: 2.2391 | Test Metrics: {'acc_top5': 79.6484375, 'acc_top1': 60.21484375}\n",
      "Client 5 - EPOCH [37/50] | Train Loss: 0.5586 | Test Loss: 2.1406 | Test Metrics: {'acc_top5': 80.29296875, 'acc_top1': 62.51953125}\n",
      "Client 5 - EPOCH [38/50] | Train Loss: 0.5449 | Test Loss: 2.1498 | Test Metrics: {'acc_top5': 80.5859375, 'acc_top1': 61.796875}\n",
      "Client 5 - EPOCH [39/50] | Train Loss: 0.5242 | Test Loss: 2.1247 | Test Metrics: {'acc_top5': 81.23046875, 'acc_top1': 62.6171875}\n",
      "Client 5 - EPOCH [40/50] | Train Loss: 0.5212 | Test Loss: 2.1879 | Test Metrics: {'acc_top5': 81.03515625, 'acc_top1': 61.46484375}\n",
      "Client 5 - EPOCH [41/50] | Train Loss: 0.5113 | Test Loss: 2.1175 | Test Metrics: {'acc_top5': 81.46484375, 'acc_top1': 63.14453125}\n",
      "Client 5 - EPOCH [42/50] | Train Loss: 0.4894 | Test Loss: 2.1690 | Test Metrics: {'acc_top5': 81.6796875, 'acc_top1': 62.1484375}\n",
      "Client 5 - EPOCH [43/50] | Train Loss: 0.4752 | Test Loss: 2.1647 | Test Metrics: {'acc_top5': 81.19140625, 'acc_top1': 62.44140625}\n",
      "Client 5 - EPOCH [44/50] | Train Loss: 0.4830 | Test Loss: 2.1959 | Test Metrics: {'acc_top5': 81.38671875, 'acc_top1': 62.59765625}\n",
      "Client 5 - EPOCH [45/50] | Train Loss: 0.4723 | Test Loss: 2.1977 | Test Metrics: {'acc_top5': 81.9140625, 'acc_top1': 61.62109375}\n",
      "Client 5 - EPOCH [46/50] | Train Loss: 0.4716 | Test Loss: 2.1346 | Test Metrics: {'acc_top5': 81.5234375, 'acc_top1': 63.90625}\n",
      "Client 5 - EPOCH [47/50] | Train Loss: 0.4805 | Test Loss: 2.1669 | Test Metrics: {'acc_top5': 82.01171875, 'acc_top1': 64.3359375}\n",
      "Client 5 - EPOCH [48/50] | Train Loss: 0.4295 | Test Loss: 2.1875 | Test Metrics: {'acc_top5': 80.64453125, 'acc_top1': 63.06640625}\n",
      "Client 5 - EPOCH [49/50] | Train Loss: 0.4549 | Test Loss: 2.1371 | Test Metrics: {'acc_top5': 81.953125, 'acc_top1': 63.37890625}\n",
      "Client 5 - EPOCH [50/50] | Train Loss: 0.4606 | Test Loss: 2.2125 | Test Metrics: {'acc_top5': 81.5234375, 'acc_top1': 63.75}\n",
      "Client 6 - EPOCH [1/50] | Train Loss: 7.3902 | Test Loss: 7.4526 | Test Metrics: {'acc_top5': 0.48828125, 'acc_top1': 0.0}\n",
      "Client 6 - EPOCH [2/50] | Train Loss: 7.1499 | Test Loss: 7.2571 | Test Metrics: {'acc_top5': 0.5859375, 'acc_top1': 0.0}\n",
      "Client 6 - EPOCH [3/50] | Train Loss: 6.5094 | Test Loss: 6.9272 | Test Metrics: {'acc_top5': 3.7109375, 'acc_top1': 0.5859375}\n",
      "Client 6 - EPOCH [4/50] | Train Loss: 5.5432 | Test Loss: 6.5319 | Test Metrics: {'acc_top5': 8.2421875, 'acc_top1': 1.2890625}\n",
      "Client 6 - EPOCH [5/50] | Train Loss: 4.7124 | Test Loss: 6.1746 | Test Metrics: {'acc_top5': 9.9609375, 'acc_top1': 3.6328125}\n",
      "Client 6 - EPOCH [6/50] | Train Loss: 4.0898 | Test Loss: 5.8890 | Test Metrics: {'acc_top5': 13.4765625, 'acc_top1': 6.38671875}\n",
      "Client 6 - EPOCH [7/50] | Train Loss: 3.5621 | Test Loss: 5.6071 | Test Metrics: {'acc_top5': 17.265625, 'acc_top1': 8.33984375}\n",
      "Client 6 - EPOCH [8/50] | Train Loss: 3.1371 | Test Loss: 5.3566 | Test Metrics: {'acc_top5': 21.62109375, 'acc_top1': 10.87890625}\n",
      "Client 6 - EPOCH [9/50] | Train Loss: 2.7926 | Test Loss: 5.1022 | Test Metrics: {'acc_top5': 25.91796875, 'acc_top1': 13.0859375}\n",
      "Client 6 - EPOCH [10/50] | Train Loss: 2.4919 | Test Loss: 4.8705 | Test Metrics: {'acc_top5': 30.625, 'acc_top1': 14.8046875}\n",
      "Client 6 - EPOCH [11/50] | Train Loss: 2.2380 | Test Loss: 4.6367 | Test Metrics: {'acc_top5': 34.6875, 'acc_top1': 17.87109375}\n",
      "Client 6 - EPOCH [12/50] | Train Loss: 2.0180 | Test Loss: 4.3933 | Test Metrics: {'acc_top5': 39.4921875, 'acc_top1': 21.8359375}\n",
      "Client 6 - EPOCH [13/50] | Train Loss: 1.8290 | Test Loss: 4.2227 | Test Metrics: {'acc_top5': 41.73828125, 'acc_top1': 24.82421875}\n",
      "Client 6 - EPOCH [14/50] | Train Loss: 1.6720 | Test Loss: 4.0118 | Test Metrics: {'acc_top5': 45.6640625, 'acc_top1': 26.9921875}\n",
      "Client 6 - EPOCH [15/50] | Train Loss: 1.5424 | Test Loss: 3.8546 | Test Metrics: {'acc_top5': 50.3125, 'acc_top1': 29.58984375}\n",
      "Client 6 - EPOCH [16/50] | Train Loss: 1.4103 | Test Loss: 3.6833 | Test Metrics: {'acc_top5': 54.19921875, 'acc_top1': 32.12890625}\n",
      "Client 6 - EPOCH [17/50] | Train Loss: 1.3139 | Test Loss: 3.4803 | Test Metrics: {'acc_top5': 58.22265625, 'acc_top1': 34.6484375}\n",
      "Client 6 - EPOCH [18/50] | Train Loss: 1.2085 | Test Loss: 3.3592 | Test Metrics: {'acc_top5': 61.2109375, 'acc_top1': 36.0546875}\n",
      "Client 6 - EPOCH [19/50] | Train Loss: 1.1311 | Test Loss: 3.1799 | Test Metrics: {'acc_top5': 64.04296875, 'acc_top1': 38.515625}\n",
      "Client 6 - EPOCH [20/50] | Train Loss: 1.0425 | Test Loss: 3.1219 | Test Metrics: {'acc_top5': 66.0546875, 'acc_top1': 39.4140625}\n",
      "Client 6 - EPOCH [21/50] | Train Loss: 0.9829 | Test Loss: 2.9342 | Test Metrics: {'acc_top5': 67.51953125, 'acc_top1': 42.05078125}\n",
      "Client 6 - EPOCH [22/50] | Train Loss: 0.9071 | Test Loss: 2.8931 | Test Metrics: {'acc_top5': 68.203125, 'acc_top1': 42.421875}\n",
      "Client 6 - EPOCH [23/50] | Train Loss: 0.8672 | Test Loss: 2.8224 | Test Metrics: {'acc_top5': 70.546875, 'acc_top1': 44.66796875}\n",
      "Client 6 - EPOCH [24/50] | Train Loss: 0.8145 | Test Loss: 2.7651 | Test Metrics: {'acc_top5': 72.16796875, 'acc_top1': 45.9765625}\n",
      "Client 6 - EPOCH [25/50] | Train Loss: 0.8114 | Test Loss: 2.6862 | Test Metrics: {'acc_top5': 73.53515625, 'acc_top1': 47.0703125}\n",
      "Client 6 - EPOCH [26/50] | Train Loss: 0.7408 | Test Loss: 2.6323 | Test Metrics: {'acc_top5': 75.05859375, 'acc_top1': 48.2421875}\n",
      "Client 6 - EPOCH [27/50] | Train Loss: 0.7267 | Test Loss: 2.6069 | Test Metrics: {'acc_top5': 76.97265625, 'acc_top1': 48.92578125}\n",
      "Client 6 - EPOCH [28/50] | Train Loss: 0.6829 | Test Loss: 2.4770 | Test Metrics: {'acc_top5': 77.4609375, 'acc_top1': 50.390625}\n",
      "Client 6 - EPOCH [29/50] | Train Loss: 0.6353 | Test Loss: 2.4302 | Test Metrics: {'acc_top5': 78.828125, 'acc_top1': 52.5}\n",
      "Client 6 - EPOCH [30/50] | Train Loss: 0.6344 | Test Loss: 2.3986 | Test Metrics: {'acc_top5': 79.12109375, 'acc_top1': 52.24609375}\n",
      "Client 6 - EPOCH [31/50] | Train Loss: 0.5643 | Test Loss: 2.4120 | Test Metrics: {'acc_top5': 79.08203125, 'acc_top1': 52.59765625}\n",
      "Client 6 - EPOCH [32/50] | Train Loss: 0.5908 | Test Loss: 2.3274 | Test Metrics: {'acc_top5': 79.921875, 'acc_top1': 53.8671875}\n",
      "Client 6 - EPOCH [33/50] | Train Loss: 0.5555 | Test Loss: 2.3959 | Test Metrics: {'acc_top5': 80.01953125, 'acc_top1': 53.4765625}\n",
      "Client 6 - EPOCH [34/50] | Train Loss: 0.5533 | Test Loss: 2.3472 | Test Metrics: {'acc_top5': 80.99609375, 'acc_top1': 54.74609375}\n",
      "Client 6 - EPOCH [35/50] | Train Loss: 0.4938 | Test Loss: 2.3761 | Test Metrics: {'acc_top5': 79.765625, 'acc_top1': 53.61328125}\n",
      "Client 6 - EPOCH [36/50] | Train Loss: 0.5325 | Test Loss: 2.2318 | Test Metrics: {'acc_top5': 80.703125, 'acc_top1': 57.20703125}\n",
      "Client 6 - EPOCH [37/50] | Train Loss: 0.5581 | Test Loss: 2.2839 | Test Metrics: {'acc_top5': 80.546875, 'acc_top1': 55.76171875}\n",
      "Client 6 - EPOCH [38/50] | Train Loss: 0.5290 | Test Loss: 2.3318 | Test Metrics: {'acc_top5': 81.09375, 'acc_top1': 55.13671875}\n",
      "Client 6 - EPOCH [39/50] | Train Loss: 0.4919 | Test Loss: 2.4822 | Test Metrics: {'acc_top5': 80.21484375, 'acc_top1': 53.61328125}\n",
      "Client 6 - EPOCH [40/50] | Train Loss: 0.4949 | Test Loss: 2.3251 | Test Metrics: {'acc_top5': 80.21484375, 'acc_top1': 56.40625}\n",
      "Client 6 - EPOCH [41/50] | Train Loss: 0.5027 | Test Loss: 2.3374 | Test Metrics: {'acc_top5': 82.03125, 'acc_top1': 55.76171875}\n",
      "Client 6 - EPOCH [42/50] | Train Loss: 0.5033 | Test Loss: 2.3665 | Test Metrics: {'acc_top5': 82.265625, 'acc_top1': 55.76171875}\n",
      "Client 6 - EPOCH [43/50] | Train Loss: 0.4721 | Test Loss: 2.3027 | Test Metrics: {'acc_top5': 81.6796875, 'acc_top1': 55.87890625}\n",
      "Client 6 - EPOCH [44/50] | Train Loss: 0.4705 | Test Loss: 2.3762 | Test Metrics: {'acc_top5': 81.34765625, 'acc_top1': 56.69921875}\n",
      "Client 6 - EPOCH [45/50] | Train Loss: 0.4457 | Test Loss: 2.3639 | Test Metrics: {'acc_top5': 81.796875, 'acc_top1': 57.65625}\n",
      "Client 6 - EPOCH [46/50] | Train Loss: 0.4612 | Test Loss: 2.3617 | Test Metrics: {'acc_top5': 81.40625, 'acc_top1': 55.17578125}\n",
      "Client 6 - EPOCH [47/50] | Train Loss: 0.4391 | Test Loss: 2.3825 | Test Metrics: {'acc_top5': 80.6640625, 'acc_top1': 56.1328125}\n",
      "Client 6 - EPOCH [48/50] | Train Loss: 0.4876 | Test Loss: 2.4902 | Test Metrics: {'acc_top5': 79.9609375, 'acc_top1': 54.140625}\n",
      "Client 6 - EPOCH [49/50] | Train Loss: 0.4628 | Test Loss: 2.3234 | Test Metrics: {'acc_top5': 82.12890625, 'acc_top1': 57.5}\n",
      "Client 6 - EPOCH [50/50] | Train Loss: 0.4493 | Test Loss: 2.4887 | Test Metrics: {'acc_top5': 80.37109375, 'acc_top1': 55.48828125}\n",
      "Client 7 - EPOCH [1/50] | Train Loss: 7.2857 | Test Loss: 7.3411 | Test Metrics: {'acc_top5': 0.5859375, 'acc_top1': 0.0}\n",
      "Client 7 - EPOCH [2/50] | Train Loss: 7.0876 | Test Loss: 7.1675 | Test Metrics: {'acc_top5': 0.83984375, 'acc_top1': 0.1953125}\n",
      "Client 7 - EPOCH [3/50] | Train Loss: 6.4380 | Test Loss: 6.7788 | Test Metrics: {'acc_top5': 5.52734375, 'acc_top1': 1.71875}\n",
      "Client 7 - EPOCH [4/50] | Train Loss: 5.4115 | Test Loss: 6.2762 | Test Metrics: {'acc_top5': 10.76171875, 'acc_top1': 4.0625}\n",
      "Client 7 - EPOCH [5/50] | Train Loss: 4.6477 | Test Loss: 5.8864 | Test Metrics: {'acc_top5': 14.1015625, 'acc_top1': 7.1875}\n",
      "Client 7 - EPOCH [6/50] | Train Loss: 4.1281 | Test Loss: 5.5640 | Test Metrics: {'acc_top5': 19.19921875, 'acc_top1': 9.296875}\n",
      "Client 7 - EPOCH [7/50] | Train Loss: 3.6178 | Test Loss: 5.2492 | Test Metrics: {'acc_top5': 24.4921875, 'acc_top1': 13.515625}\n",
      "Client 7 - EPOCH [8/50] | Train Loss: 3.1641 | Test Loss: 4.9414 | Test Metrics: {'acc_top5': 28.49609375, 'acc_top1': 17.44140625}\n",
      "Client 7 - EPOCH [9/50] | Train Loss: 2.7580 | Test Loss: 4.6390 | Test Metrics: {'acc_top5': 34.4921875, 'acc_top1': 20.17578125}\n",
      "Client 7 - EPOCH [10/50] | Train Loss: 2.4045 | Test Loss: 4.3660 | Test Metrics: {'acc_top5': 39.55078125, 'acc_top1': 24.04296875}\n",
      "Client 7 - EPOCH [11/50] | Train Loss: 2.1179 | Test Loss: 4.0985 | Test Metrics: {'acc_top5': 44.55078125, 'acc_top1': 26.93359375}\n",
      "Client 7 - EPOCH [12/50] | Train Loss: 1.8967 | Test Loss: 3.8851 | Test Metrics: {'acc_top5': 50.0390625, 'acc_top1': 30.41015625}\n",
      "Client 7 - EPOCH [13/50] | Train Loss: 1.7202 | Test Loss: 3.6567 | Test Metrics: {'acc_top5': 54.12109375, 'acc_top1': 33.96484375}\n",
      "Client 7 - EPOCH [14/50] | Train Loss: 1.5512 | Test Loss: 3.4621 | Test Metrics: {'acc_top5': 57.83203125, 'acc_top1': 36.71875}\n",
      "Client 7 - EPOCH [15/50] | Train Loss: 1.4015 | Test Loss: 3.2609 | Test Metrics: {'acc_top5': 61.5625, 'acc_top1': 41.1328125}\n",
      "Client 7 - EPOCH [16/50] | Train Loss: 1.2886 | Test Loss: 3.1195 | Test Metrics: {'acc_top5': 65.3515625, 'acc_top1': 43.5546875}\n",
      "Client 7 - EPOCH [17/50] | Train Loss: 1.1503 | Test Loss: 3.0145 | Test Metrics: {'acc_top5': 66.6796875, 'acc_top1': 44.296875}\n",
      "Client 7 - EPOCH [18/50] | Train Loss: 1.0900 | Test Loss: 2.9000 | Test Metrics: {'acc_top5': 68.0078125, 'acc_top1': 46.93359375}\n",
      "Client 7 - EPOCH [19/50] | Train Loss: 0.9918 | Test Loss: 2.7873 | Test Metrics: {'acc_top5': 68.88671875, 'acc_top1': 50.078125}\n",
      "Client 7 - EPOCH [20/50] | Train Loss: 0.9483 | Test Loss: 2.6581 | Test Metrics: {'acc_top5': 71.1328125, 'acc_top1': 52.03125}\n",
      "Client 7 - EPOCH [21/50] | Train Loss: 0.8972 | Test Loss: 2.6428 | Test Metrics: {'acc_top5': 71.2890625, 'acc_top1': 52.55859375}\n",
      "Client 7 - EPOCH [22/50] | Train Loss: 0.7967 | Test Loss: 2.5190 | Test Metrics: {'acc_top5': 73.30078125, 'acc_top1': 53.5546875}\n",
      "Client 7 - EPOCH [23/50] | Train Loss: 0.7949 | Test Loss: 2.4398 | Test Metrics: {'acc_top5': 74.82421875, 'acc_top1': 54.47265625}\n",
      "Client 7 - EPOCH [24/50] | Train Loss: 0.7430 | Test Loss: 2.3782 | Test Metrics: {'acc_top5': 76.23046875, 'acc_top1': 55.5078125}\n",
      "Client 7 - EPOCH [25/50] | Train Loss: 0.7107 | Test Loss: 2.2917 | Test Metrics: {'acc_top5': 76.38671875, 'acc_top1': 58.046875}\n",
      "Client 7 - EPOCH [26/50] | Train Loss: 0.6627 | Test Loss: 2.2704 | Test Metrics: {'acc_top5': 76.58203125, 'acc_top1': 57.94921875}\n",
      "Client 7 - EPOCH [27/50] | Train Loss: 0.6582 | Test Loss: 2.1874 | Test Metrics: {'acc_top5': 77.265625, 'acc_top1': 60.2734375}\n",
      "Client 7 - EPOCH [28/50] | Train Loss: 0.6350 | Test Loss: 2.1673 | Test Metrics: {'acc_top5': 79.1796875, 'acc_top1': 60.2734375}\n",
      "Client 7 - EPOCH [29/50] | Train Loss: 0.6140 | Test Loss: 2.1445 | Test Metrics: {'acc_top5': 79.23828125, 'acc_top1': 60.7421875}\n",
      "Client 7 - EPOCH [30/50] | Train Loss: 0.5710 | Test Loss: 2.0943 | Test Metrics: {'acc_top5': 81.6015625, 'acc_top1': 59.8828125}\n",
      "Client 7 - EPOCH [31/50] | Train Loss: 0.5623 | Test Loss: 2.0654 | Test Metrics: {'acc_top5': 81.640625, 'acc_top1': 61.5625}\n",
      "Client 7 - EPOCH [32/50] | Train Loss: 0.5196 | Test Loss: 2.0379 | Test Metrics: {'acc_top5': 82.578125, 'acc_top1': 61.6015625}\n",
      "Client 7 - EPOCH [33/50] | Train Loss: 0.5448 | Test Loss: 2.0079 | Test Metrics: {'acc_top5': 82.1875, 'acc_top1': 62.36328125}\n",
      "Client 7 - EPOCH [34/50] | Train Loss: 0.5551 | Test Loss: 1.9913 | Test Metrics: {'acc_top5': 83.0078125, 'acc_top1': 63.203125}\n",
      "Client 7 - EPOCH [35/50] | Train Loss: 0.5618 | Test Loss: 1.9484 | Test Metrics: {'acc_top5': 83.41796875, 'acc_top1': 64.296875}\n",
      "Client 7 - EPOCH [36/50] | Train Loss: 0.5371 | Test Loss: 1.9760 | Test Metrics: {'acc_top5': 82.96875, 'acc_top1': 62.12890625}\n",
      "Client 7 - EPOCH [37/50] | Train Loss: 0.5466 | Test Loss: 1.9521 | Test Metrics: {'acc_top5': 83.3203125, 'acc_top1': 63.6328125}\n",
      "Client 7 - EPOCH [38/50] | Train Loss: 0.4958 | Test Loss: 1.9490 | Test Metrics: {'acc_top5': 83.69140625, 'acc_top1': 64.765625}\n",
      "Client 7 - EPOCH [39/50] | Train Loss: 0.4779 | Test Loss: 1.9381 | Test Metrics: {'acc_top5': 83.75, 'acc_top1': 65.33203125}\n",
      "Client 7 - EPOCH [40/50] | Train Loss: 0.4721 | Test Loss: 1.9236 | Test Metrics: {'acc_top5': 83.80859375, 'acc_top1': 64.3359375}\n",
      "Client 7 - EPOCH [41/50] | Train Loss: 0.4930 | Test Loss: 1.9082 | Test Metrics: {'acc_top5': 84.82421875, 'acc_top1': 63.92578125}\n",
      "Client 7 - EPOCH [42/50] | Train Loss: 0.4744 | Test Loss: 1.9527 | Test Metrics: {'acc_top5': 83.33984375, 'acc_top1': 64.04296875}\n",
      "Client 7 - EPOCH [43/50] | Train Loss: 0.4517 | Test Loss: 1.8835 | Test Metrics: {'acc_top5': 84.3359375, 'acc_top1': 65.72265625}\n",
      "Client 7 - EPOCH [44/50] | Train Loss: 0.4563 | Test Loss: 1.9789 | Test Metrics: {'acc_top5': 83.45703125, 'acc_top1': 63.90625}\n",
      "Client 7 - EPOCH [45/50] | Train Loss: 0.4630 | Test Loss: 1.9100 | Test Metrics: {'acc_top5': 84.04296875, 'acc_top1': 64.296875}\n",
      "Client 7 - EPOCH [46/50] | Train Loss: 0.4281 | Test Loss: 1.9135 | Test Metrics: {'acc_top5': 84.375, 'acc_top1': 65.234375}\n",
      "Client 7 - EPOCH [47/50] | Train Loss: 0.4393 | Test Loss: 2.0404 | Test Metrics: {'acc_top5': 83.14453125, 'acc_top1': 64.58984375}\n",
      "Client 7 - EPOCH [48/50] | Train Loss: 0.4066 | Test Loss: 1.9758 | Test Metrics: {'acc_top5': 84.04296875, 'acc_top1': 65.8984375}\n",
      "Client 7 - EPOCH [49/50] | Train Loss: 0.4169 | Test Loss: 1.9756 | Test Metrics: {'acc_top5': 84.23828125, 'acc_top1': 64.86328125}\n",
      "Client 7 - EPOCH [50/50] | Train Loss: 0.4433 | Test Loss: 1.9875 | Test Metrics: {'acc_top5': 82.96875, 'acc_top1': 64.98046875}\n",
      "Client 8 - EPOCH [1/50] | Train Loss: 7.3937 | Test Loss: 7.3822 | Test Metrics: {'acc_top5': 0.5859375, 'acc_top1': 0.0}\n",
      "Client 8 - EPOCH [2/50] | Train Loss: 7.1991 | Test Loss: 7.2259 | Test Metrics: {'acc_top5': 0.390625, 'acc_top1': 0.0}\n",
      "Client 8 - EPOCH [3/50] | Train Loss: 6.6456 | Test Loss: 6.9403 | Test Metrics: {'acc_top5': 2.83203125, 'acc_top1': 1.46484375}\n",
      "Client 8 - EPOCH [4/50] | Train Loss: 5.7011 | Test Loss: 6.4886 | Test Metrics: {'acc_top5': 10.1171875, 'acc_top1': 4.4921875}\n",
      "Client 8 - EPOCH [5/50] | Train Loss: 4.7008 | Test Loss: 6.0234 | Test Metrics: {'acc_top5': 14.55078125, 'acc_top1': 7.51953125}\n",
      "Client 8 - EPOCH [6/50] | Train Loss: 4.0106 | Test Loss: 5.6211 | Test Metrics: {'acc_top5': 20.56640625, 'acc_top1': 10.46875}\n",
      "Client 8 - EPOCH [7/50] | Train Loss: 3.4311 | Test Loss: 5.2745 | Test Metrics: {'acc_top5': 27.67578125, 'acc_top1': 15.13671875}\n",
      "Client 8 - EPOCH [8/50] | Train Loss: 2.9357 | Test Loss: 4.9335 | Test Metrics: {'acc_top5': 34.47265625, 'acc_top1': 18.8671875}\n",
      "Client 8 - EPOCH [9/50] | Train Loss: 2.5348 | Test Loss: 4.6303 | Test Metrics: {'acc_top5': 39.9609375, 'acc_top1': 23.92578125}\n",
      "Client 8 - EPOCH [10/50] | Train Loss: 2.1636 | Test Loss: 4.3340 | Test Metrics: {'acc_top5': 44.27734375, 'acc_top1': 28.6328125}\n",
      "Client 8 - EPOCH [11/50] | Train Loss: 1.8692 | Test Loss: 4.0892 | Test Metrics: {'acc_top5': 48.92578125, 'acc_top1': 32.71484375}\n",
      "Client 8 - EPOCH [12/50] | Train Loss: 1.6606 | Test Loss: 3.8400 | Test Metrics: {'acc_top5': 51.9140625, 'acc_top1': 36.25}\n",
      "Client 8 - EPOCH [13/50] | Train Loss: 1.4750 | Test Loss: 3.6311 | Test Metrics: {'acc_top5': 54.66796875, 'acc_top1': 38.80859375}\n",
      "Client 8 - EPOCH [14/50] | Train Loss: 1.3132 | Test Loss: 3.4255 | Test Metrics: {'acc_top5': 58.8671875, 'acc_top1': 41.4453125}\n",
      "Client 8 - EPOCH [15/50] | Train Loss: 1.1691 | Test Loss: 3.2452 | Test Metrics: {'acc_top5': 60.72265625, 'acc_top1': 45.078125}\n",
      "Client 8 - EPOCH [16/50] | Train Loss: 1.1134 | Test Loss: 3.0918 | Test Metrics: {'acc_top5': 62.59765625, 'acc_top1': 48.5546875}\n",
      "Client 8 - EPOCH [17/50] | Train Loss: 1.0256 | Test Loss: 2.9420 | Test Metrics: {'acc_top5': 65.52734375, 'acc_top1': 50.91796875}\n",
      "Client 8 - EPOCH [18/50] | Train Loss: 0.9283 | Test Loss: 2.8271 | Test Metrics: {'acc_top5': 67.63671875, 'acc_top1': 52.87109375}\n",
      "Client 8 - EPOCH [19/50] | Train Loss: 0.8781 | Test Loss: 2.7063 | Test Metrics: {'acc_top5': 70.91796875, 'acc_top1': 54.296875}\n",
      "Client 8 - EPOCH [20/50] | Train Loss: 0.8388 | Test Loss: 2.5776 | Test Metrics: {'acc_top5': 73.0078125, 'acc_top1': 56.40625}\n",
      "Client 8 - EPOCH [21/50] | Train Loss: 0.7695 | Test Loss: 2.5000 | Test Metrics: {'acc_top5': 74.43359375, 'acc_top1': 57.1484375}\n",
      "Client 8 - EPOCH [22/50] | Train Loss: 0.7326 | Test Loss: 2.3628 | Test Metrics: {'acc_top5': 76.54296875, 'acc_top1': 59.6484375}\n",
      "Client 8 - EPOCH [23/50] | Train Loss: 0.6899 | Test Loss: 2.2912 | Test Metrics: {'acc_top5': 77.12890625, 'acc_top1': 60.29296875}\n",
      "Client 8 - EPOCH [24/50] | Train Loss: 0.6852 | Test Loss: 2.2607 | Test Metrics: {'acc_top5': 78.88671875, 'acc_top1': 61.03515625}\n",
      "Client 8 - EPOCH [25/50] | Train Loss: 0.6182 | Test Loss: 2.1855 | Test Metrics: {'acc_top5': 79.5703125, 'acc_top1': 61.796875}\n",
      "Client 8 - EPOCH [26/50] | Train Loss: 0.6066 | Test Loss: 2.1477 | Test Metrics: {'acc_top5': 80.64453125, 'acc_top1': 62.5390625}\n"
     ]
    }
   ],
   "source": [
    "# Define training tracking storage\n",
    "training_log = {}\n",
    "\n",
    "# Training loop for each dataset\n",
    "for client_id, (train_set, test_set) in enumerate(dataset_all_clients):\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_set, num_workers=4, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_set, num_workers=4, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Load pretrained model and update classification head\n",
    "    vit_model = load_pretrained_vit(NUM_CLASSES).to(DEVICE)\n",
    "    \n",
    "    # Optimizer with layer-wise learning rate decay\n",
    "    optimizer = AdamW(vit_model.parameters(),lr=LR,weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    # Scheduler with warmup\n",
    "    num_training_steps = len(train_loader) * EPOCHS\n",
    "    scheduler = get_linear_scheduler(optimizer, num_warmup_steps=int(0.1 * num_training_steps), num_training_steps=num_training_steps)\n",
    "    \n",
    "    # Loss criterion\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize logging for this dataset\n",
    "    training_log[client_id] = {'train_loss': [], 'test_loss': [], 'test_metrics': []}\n",
    "\n",
    "    # Fine-tuning loop\n",
    "    for ep in range(EPOCHS):\n",
    "        # Train\n",
    "        train_loss = train(vit_model, criterion, train_loader, optimizer, 1, DEVICE)\n",
    "        \n",
    "        # Evaluate\n",
    "        test_loss, test_metrics = evalulate(vit_model, test_loader, DEVICE)\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Record losses and metrics\n",
    "        training_log[client_id]['train_loss'].append(train_loss)\n",
    "        training_log[client_id]['test_loss'].append(test_loss)\n",
    "        training_log[client_id]['test_metrics'].append(test_metrics)\n",
    "\n",
    "        # Print logging info\n",
    "        print(f\"Client {client_id} - EPOCH [{ep + 1}/{EPOCHS}] | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Test Metrics: {test_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prepare a list to hold the structured data for the DataFrame\n",
    "results = []\n",
    "\n",
    "# Populate the list with each client's results\n",
    "for client_id, logs in training_log.items():\n",
    "    for epoch in range(EPOCHS):\n",
    "        results.append({\n",
    "            'client_id': client_id,\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': logs['train_loss'][epoch],\n",
    "            'test_loss': logs['test_loss'][epoch],\n",
    "            'test_metrics': logs['test_metrics'][epoch]\n",
    "        })\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "results_df.to_csv(\"training_results.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dual_ligo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
